{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBrJtH2IA_xq"
   },
   "source": [
    "__NAME:__ __FULLNAME__  \n",
    "__SECTION:__ __NUMBER__  \n",
    "__CS 5703: Machine Learning Practices__\n",
    "\n",
    "# Homework 7: Model Comparisons\n",
    "\n",
    "## Assignment Overview\n",
    "Generally, it's helpful to first read through the entire notebook before writing\n",
    "any code to obtain a sense of the overall program structure before you start coding.  \n",
    "\n",
    "Follow the TODOs and read through and understand any provided code.  \n",
    "\n",
    "\n",
    "### Task\n",
    "For this assignment, you'll be comparing different models after performing holistic\n",
    "cross validation to find the best parameter sets for various sizes of the training\n",
    "data. \n",
    "\n",
    "For this assignment, we will try to predict shoulder and elbow torque simultaneously,\n",
    "from the neural activation. \n",
    "\n",
    "### Data set\n",
    "The BMI (Brain Machine Interface) data are stored in a single pickle file; within this file, there\n",
    "is one dictionary that contains all of the data.  The keys are: 'MI', \n",
    "'theta', 'dtheta', 'torque', and 'time'.  Each of these objects are python lists with 20 \n",
    "numpy matrices; each matrix contains an independent fold of data, with rows representing \n",
    "different samples and columns representing different dimensions.  The samples are organized \n",
    "contiguously (one sample every 50ms), but there are gaps in the data.\n",
    "* _MI_ contains the data for 48 neurons.  Each row encodes the number of action potentials for \n",
    "each neuron at each of 20 different time bins (so, 48 x 20 = 960 columns).  \n",
    "* _theta_ contains the angular position of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each sample.  \n",
    "* _dtheta_ contains the angular velocity of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each sample.  \n",
    "* _torque_ contains the torque of the shoulder (in column 0) and the elbow (in column  1) for each sample.  \n",
    "* _time_ contains the actual time stamp of each sample.\n",
    "\n",
    "A fold is a subset of the available data.  Cutting the data into folds is useful for adjusting training, validation, and test \n",
    "sets sizes, and for assessing the generality of a modelling approach.Each fold contains independent time points.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "* Understanding hyper-parameter selection using __holistic cross validation__\n",
    "* Understanding how training set size affects model choices\n",
    "* Learning to perform model selection using hypothesis testing\n",
    "\n",
    "### Notes\n",
    "* Be sure to adequately label all the plots you generate.\n",
    "* Make sure to save your notebook and results into your own Google Drive\n",
    "\n",
    "### General References\n",
    "* [Guide to Jupyter](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Python Built-in Functions](https://docs.python.org/3/library/functions.html)\n",
    "* [Python Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "* [Numpy Reference](https://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "* [Numpy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [Summary of matplotlib](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* [DataCamp: Matplotlib](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264365&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=9026223&gclid=CjwKCAjw_uDsBRAMEiwAaFiHa8xhgCsO9wVcuZPGjAyVGTitb_-fxYtkBLkQ4E_GjSCZFVCqYCGkphoCjucQAvD_BwE)\n",
    "* [Pandas DataFrames](https://urldefense.proofpoint.com/v2/url?u=https-3A__pandas.pydata.org_pandas-2Ddocs_stable_reference_api_pandas.DataFrame.html&d=DwMD-g&c=qKdtBuuu6dQK9MsRUVJ2DPXW6oayO8fu4TfEHS8sGNk&r=9ngmsG8rSmDSS-O0b_V0gP-nN_33Vr52qbY3KXuDY5k&m=mcOOc8D0knaNNmmnTEo_F_WmT4j6_nUSL_yoPmGlLWQ&s=h7hQjqucR7tZyfZXxnoy3iitIr32YlrqiFyPATkW3lw&e=)\n",
    "* [Sci-kit Learn Linear Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Sci-kit Learn Ensemble Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Sci-kit Learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Sci-kit Learn Model Selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "* [SciPy Paired t-test for Dependent Samples](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html)\n",
    "* [Student's t-test](https://en.wikipedia.org/wiki/Student%27s_t-test#Dependent_t-test_for_paired_samples)\n",
    "* [Understanding Paired t-tests](https://machinelearningmastery.com/how-to-code-the-students-t-test-from-scratch-in-python/)\n",
    "* [Two-tailed Tests](https://www.investopedia.com/terms/t/two-tailed-test.asp)\n",
    "\n",
    "## Hand-In Procedure\n",
    "* Execute all cells so they are showing correct results\n",
    "* Notebook (from Jupyter or Colab):\n",
    "  + Submit this file (.ipynb) to the Gradescope Notebook HW7 dropbox\n",
    "* Note: there is no need to submit a PDF file or to submit directly to Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAeoqbCtA_x3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import pathlib, itertools\n",
    "import time as timelib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKSWmWt8A_x8",
    "outputId": "ba886d20-26ab-4058-c3c9-556fd8cbf304"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqdlttxGA_yE"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LVL0XJgA_yG",
    "outputId": "6acedf2a-bf2b-4e75-9998-5eb0f7ca7de9"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Load the BMI data from all the folds\n",
    "\"\"\"\n",
    "#fname = '/home/fagg/datasets/bmi/bmi_dataset.pkl'\n",
    "fname = '/content/drive/MyDrive/MLP_2022/datasets/bmi_dataset.pkl'\n",
    "\n",
    "with open(fname, 'rb') as f:\n",
    "  bmi = pkl.load(f)\n",
    "\n",
    "MI_folds = bmi['MI'] \n",
    "theta_folds = bmi['theta']\n",
    "dtheta_folds = bmi['dtheta']\n",
    "torque_folds = bmi['torque']\n",
    "time_folds = bmi['time']\n",
    "\n",
    "alldata_folds = zip(MI_folds, theta_folds, dtheta_folds, torque_folds, time_folds)\n",
    "\n",
    "nfolds = len(MI_folds)\n",
    "nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzIz0W-YA_yI",
    "outputId": "5306bc7b-1bde-4a85-e056-1c3ecf309fb0"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Print out the shape of all the data for each fold\n",
    "\"\"\"\n",
    "for i, (MI, theta, dtheta, torque, time) in enumerate(alldata_folds):\n",
    "    print(\"FOLD %2d \" % i, MI.shape, theta.shape, \n",
    "          dtheta.shape, torque.shape, time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vNtnO-OA_yJ",
    "outputId": "a17620b3-997b-4aea-9231-7f824cf537f8"
   },
   "outputs": [],
   "source": [
    "\"\"\" Provided\n",
    "For this homework, it actually becomes rather difficult to work with \n",
    "individual arrays of different length, because numpy cannot efficiently\n",
    "concatenate them together  \n",
    "\n",
    "Here, we construct a single unified array for our data, which we can \n",
    "then be indexed into using the folds_idx array. This allows us to efficiently \n",
    "index into our data instead of creating\n",
    "a copy of it each time we want to change the size of the the training set or\n",
    "our cross-validation rotation.\n",
    "\"\"\"\n",
    "folds_idx = [None]*nfolds\n",
    "unified_idx = 0\n",
    "for i, fold in enumerate(time_folds):\n",
    "    # creates a list containing indexes from start of fold to end of fold,\n",
    "    # eg folds_idx[0] = [0,1,...,1192], folds_idx[1] = [1193,...,2296], ...\n",
    "    # we don't need to store all this (could just store start indexes),\n",
    "    # but this makes it a lot easier later\n",
    "    folds_idx[i] = list(range(unified_idx, unified_idx + fold.shape[0]))\n",
    "    unified_idx += fold.shape[0]\n",
    "\n",
    "def concat_folds(folds):\n",
    "    all_folds = np.array(folds[0])\n",
    "    for f in folds[1:]:\n",
    "        all_folds = np.append(all_folds, f, axis=0)\n",
    "    return all_folds\n",
    "\n",
    "MI = concat_folds(MI_folds) \n",
    "theta = concat_folds(theta_folds)\n",
    "dtheta = concat_folds(dtheta_folds)\n",
    "torque = concat_folds(torque_folds)\n",
    "time = concat_folds(time_folds)\n",
    "\n",
    "print(MI.shape, theta.shape, dtheta.shape, torque.shape, time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f[0] for f in folds_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AA0h3x6A_yK"
   },
   "source": [
    "# PARAMETER SET LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-tpiCW_A_yL"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Construct the Cartesian product of the parameters.\n",
    "\n",
    "This implementation is similar to what is used by GridSearchCV to create its\n",
    "Cartesian product of hyper-parameters.\n",
    "\n",
    "Note that one can also include other hyper-parameters in the list with only a single\n",
    "option.  This has the effect of simply setting the same hyper-parameter value for each\n",
    "of resulting hyper-parameter set.\n",
    "\n",
    "\"\"\"\n",
    "def generate_paramsets(param_lists):\n",
    "    '''\n",
    "    Construct the Cartesian product of the parameters\n",
    "    PARAMS:\n",
    "        params_lists: dict of lists of values to try for each parameter.\n",
    "                      keys of the dict are the names of the parameters\n",
    "                      values are lists of values to try for the \n",
    "                      corresponding parameter\n",
    "    RETURNS: a list of dicts of hyper-parameter sets.  These make up the \n",
    "    Cartesian product of the possible hyper-parameters\n",
    "    '''\n",
    "    keys, values = zip(*param_lists.items())\n",
    "    \n",
    "    # Determines cartesian product of parameter values\n",
    "    combos = itertools.product(*values)\n",
    "    \n",
    "    # Constructs list of dictionaries\n",
    "    combos_dicts = [dict(zip(keys, vals)) for vals in combos]\n",
    "    \n",
    "    return list(combos_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2ltUG2FA_yN"
   },
   "source": [
    "# PERFORMANCE EVALUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQqcTrJtA_yQ"
   },
   "outputs": [],
   "source": [
    "def mse_rmse(trues, preds):\n",
    "    ''' PROVIDED\n",
    "    Compute MSE and rMSE for each column separately.\n",
    "    '''\n",
    "    mse = np.sum(np.square(trues - preds), axis=0) / trues.shape[0]\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse, rmse\n",
    "\n",
    "\n",
    "def score_eval(X, y, preds):\n",
    "    '''\n",
    "    Compute the  performance scores for the reulsts produced by a model\n",
    "    PARAMS:\n",
    "        X: input feature data\n",
    "        y: true output for X\n",
    "        preds: predicted output for X\n",
    "    RETURNS: results as a dictionary of numpy arrays\n",
    "        mse: mean squared error for each column\n",
    "        rmse: rMSE for each column\n",
    "        fvaf: Fraction of Variance Accounted For, best is 1.0\n",
    "    '''\n",
    "\n",
    "    mse, rmse = mse_rmse(y, preds)\n",
    "    evar = explained_variance_score(y, preds)\n",
    "    \n",
    "    #  This is a \n",
    "    # dictionary of numpy arrays. The numpy arrays are\n",
    "    # be row vectors, where each element is the result \n",
    "    # for a different output, when using multiple regression.\n",
    "    # The keys of the dictionary are the name of the performance \n",
    "    # metric, and the values are the numpy row vectors\n",
    "    results = {\n",
    "        'mse'  : np.reshape(mse,  (1, -1)), \n",
    "        'rmse' : np.reshape(rmse, (1, -1)), \n",
    "        'fvaf' : np.reshape(evar, (1, -1)),  # Fraction of Variance Accounted For\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fs4rFKtMA_yU"
   },
   "source": [
    "# CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPfQmieeA_yV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PROVIDED\n",
    "\n",
    "Modified from HW 6\n",
    "\"\"\"\n",
    "class KFoldHolisticCrossValidation():\n",
    "    '''\n",
    "    Cross-validation class. This class will perform cross-validation across for a \n",
    "    single hyper-parameter set.\n",
    "    '''\n",
    "    def __init__(self, model, eval_func, rotation_skip=1): #opt_metric, \n",
    "                #maximize_opt_metric=False, rotation_skip=1):\n",
    "        '''\n",
    "        :param model: The Scikit-Learn model to be trained\n",
    "        :param eval_func': Python function that will be used to evaluate a model\n",
    "                                parameters: (inputs, desired outputs, model predictions)\n",
    "        :param rotation_skip: Number of CV rotations for every one rotation to train & evaluate.  \n",
    "                                Typical is 1 (train and evaluate all rotations), but when we are \n",
    "                                debugging, it is helpful to perform a smaller number of train/evaluate\n",
    "                                cycles\n",
    "        '''\n",
    "        # TODO: set the class variables\n",
    "        self.model = model\n",
    "        #self.model = #TODO\n",
    "        self.eval_func = eval_func\n",
    "        #self.eval_func = #TODO\n",
    "        self.rotation_skip = rotation_skip\n",
    "\n",
    "    def perform_cross_validation(self, X, y, folds_idx, trainsize):\n",
    "        ''' This is where the bulk of the work will be done\n",
    "        Perform cross validation for a singular train set size and single \n",
    "        hyper-parameter set, by evaluating the model's performance over \n",
    "        multiple data set rotations all of the same size.\n",
    "\n",
    "        NOTE: This function assumes the hyper-parameters have already been \n",
    "              set in the model\n",
    "            \n",
    "        PARAMS:\n",
    "            X: numpy array containing all of the input data \n",
    "               (folds concatenated together)\n",
    "            y: numpy array containing all of the output data \n",
    "               (folds concatenated together)\n",
    "            folds_idx: list of lists containing the indexes\n",
    "                       of each element in each fold, eg\n",
    "                       folds_idx[i] = [start_idx, ... , end_idx]\n",
    "            trainsize: number of folds to use for training\n",
    "            \n",
    "        RETURNS: train, val, and test set results for all rotations of the  \n",
    "                 data sets and the summary (i.e., the averages over all the \n",
    "                 rotations) of the results. \n",
    "                 \n",
    "                 results is a dictionary of dictionaries of r-by-n numpy \n",
    "                 arrays, where r is the number of rotations, and n is the \n",
    "                 number of outputs from the model.\n",
    "                 \n",
    "                 summary is a dict of dictionaries of 1-by-n numpy arrays containing\n",
    "                 the mean and standard deviation of the metrics in results across\n",
    "                 all rotations\n",
    "                 \n",
    "                 In our BMI dataset, n = 2 (e.g., shoulder torque and elbow torque)\n",
    "\n",
    "                 General form:\n",
    "                     results.keys() = ['train', 'val', 'test']\n",
    "\n",
    "                     results['train'].keys() = ['metric1', 'metric2', ...]\n",
    "                     \n",
    "                     results['train']['metric1'] = numpy_array\n",
    "                     \n",
    "                     results = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse' : r_by_n_numpy_array,\n",
    "                                     'rmse': r_by_n_numpy_array, \n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "                     \n",
    "                     summary = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse_mean' : 1_by_n_numpy_array,\n",
    "                                     'mse_std'  : 1_by_n_numpy_array,\n",
    "                                     'rmse_mean': 1_by_n_numpy_array, \n",
    "                                     'rmse_std' : 1_by_n_numpy_array,\n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "\n",
    "                    For example, you can access the MSE results for the \n",
    "                    validation set like so:\n",
    "                        results['val']['mse'] \n",
    "                    For example, you can access the summary (i.e. the average \n",
    "                    results over all the rotations) for the test set for the\n",
    "                    rMSE like so:\n",
    "                        summary['test']['rmse_mean']                \n",
    "        '''\n",
    "        \n",
    "        # Verify that a valid train set size was provided\n",
    "        nfolds = len(folds_idx)\n",
    "        if trainsize > nfolds - 2: \n",
    "            err_msg = \"ERROR: KFoldHolisticCrossValidation.perform_cross_validation() - \"\n",
    "            err_msg += \"trainsize (%d) cant be more than nfolds (%d) - 2\" % (trainsize, nfolds)\n",
    "            raise ValueError(err_msg)\n",
    "        \n",
    "        # Set up results data structures for each rotation\n",
    "        results = {'train': None, 'val': None, 'test': None}\n",
    "        summary = {'train': {}, 'val': {}, 'test': {}}\n",
    "        \n",
    "        model = self.model\n",
    "        evaluate = self.eval_func\n",
    "        \n",
    "        # Rotate through different train, val, and test sets\n",
    "        for rotation in range(0, nfolds, self.rotation_skip):\n",
    "            (\n",
    "                Xtrain, ytrain, Xval, yval,  Xtest, ytest\n",
    "            ) = self.get_data(X, y, folds_idx, nfolds, rotation, trainsize)\n",
    "\n",
    "            print(ytrain.shape)\n",
    "            \n",
    "            # Train model using the training set\n",
    "            model.fit(Xtrain, ytrain) # make sure warm_start is False\n",
    "\n",
    "            # Predict with the model for train, val, and test sets\n",
    "            preds = model.predict(Xtrain)\n",
    "            preds_val = model.predict(Xval)\n",
    "            preds_test = model.predict(Xtest)\n",
    "\n",
    "            # Evaluate the model for each set\n",
    "            res_train = evaluate(Xtrain, ytrain, preds)\n",
    "            res_val = evaluate(Xval, yval, preds_val)\n",
    "            res_test = evaluate(Xtest, ytest, preds_test)\n",
    "\n",
    "            # Record the train, val, and test set results. These are dicts \n",
    "            # of result metrics, returned by the evaluate function\n",
    "\n",
    "            if results['train'] is None: \n",
    "                # First rotation: initialize structures\n",
    "                results['train'] = res_train\n",
    "                results['val'] = res_val\n",
    "                results['test'] = res_test\n",
    "            else:\n",
    "                # Other rotations: add results to existing structures\n",
    "                for metric in res_train.keys():\n",
    "                    results['train'][metric] = np.append(results['train'][metric], res_train[metric], axis=0)\n",
    "                    results['val'][metric] = np.append(results['val'][metric], res_val[metric], axis=0)\n",
    "                    results['test'][metric] = np.append(results['test'][metric], res_test[metric], axis=0)\n",
    "\n",
    "        # Compute/record mean and standard deviation for the size for each metric\n",
    "        #  The mean is across the rotations\n",
    "        for metric in results['train'].keys():\n",
    "            for stat_set in ['train', 'val', 'test']:\n",
    "                summary[stat_set][metric+'_mean'] = np.mean(results[stat_set][metric], \n",
    "                                                            axis=0).reshape(1, -1)\n",
    "                summary[stat_set][metric+'_std'] = np.std(results[stat_set][metric], \n",
    "                                                          axis=0).reshape(1, -1)\n",
    "\n",
    "        return results, summary\n",
    "\n",
    "    def get_data(self, X, y, folds_idx, nfolds, rotation, trainsize):\n",
    "        '''\n",
    "        Determines the fold indices for the train, val, and test set given\n",
    "        the total number of folds, rotation, and training set size.\n",
    "        Use these fold indices to get the training, validation, and test sets\n",
    "        from all_xfolds and all_folds\n",
    "        '''\n",
    "        # Determine folds to use \n",
    "        # (eg fold 1,2,3 for trainsize=3, rotation=1, nfolds=20)\n",
    "        trainfolds = (np.arange(trainsize) + rotation) % nfolds\n",
    "        valfold = (nfolds - 2 + rotation) % nfolds\n",
    "        testfold = (valfold + 1) % nfolds\n",
    "        \n",
    "        # Construct a list to serve as an index into X for our training\n",
    "        # samples. This will contain the index of each sample the training set\n",
    "        train_idx = []\n",
    "        for i in trainfolds:\n",
    "            # the + operator concatenates raw python lists\n",
    "            train_idx += folds_idx[i] \n",
    "\n",
    "        # Construct train set by indexing into X and y with the indices of the\n",
    "        #  samples that belong to the training set\n",
    "        Xtrain = X[train_idx]\n",
    "        ytrain = y[train_idx]\n",
    "        \n",
    "        # Construct validation set using the valfold.\n",
    "        Xval = X[folds_idx[valfold]]\n",
    "        yval = y[folds_idx[valfold]]\n",
    "\n",
    "        # Construct test set using the testfold\n",
    "        Xtest = X[folds_idx[testfold]]\n",
    "        ytest = y[folds_idx[testfold]]\n",
    "        \n",
    "        return Xtrain, ytrain, Xval, yval, Xtest, ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOJT4GHNA_yW"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PROVIDED\n",
    "\n",
    "Modified from HW 6\n",
    "\"\"\"\n",
    "\n",
    "class CrossValidationGridSearch():\n",
    "    '''\n",
    "    This class is responsible for performing a grid trainsizes x paramsets CV experiments.\n",
    "    For each grid point, N-fold crossvalidation is performed (with potential skips in the\n",
    "    possible rotations).\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, model, paramsets, eval_func, opt_metric, \n",
    "                 opt_metric_scale = 1,\n",
    "                 maximize_opt_metric=False, trainsizes=[1], \n",
    "                 rotation_skip=1,\n",
    "                ):\n",
    "        ''' \n",
    "        Class instance constructor\n",
    "        \n",
    "        :param model: Model to be learned\n",
    "        :param paramsets: List of dicts.  Every dict contains a set of hyper-parameters for use in\n",
    "                            one experiment\n",
    "        :param eval_func: Python function that will be used to evaluate a model\n",
    "                                parameters: (inputs, desired outputs, model predictions)\n",
    "        :param opt_metric: Optimization metric to be used\n",
    "        :param opt_metric_scale: Scale factor for optimization metric.  Used for plotting\n",
    "        :param maximize_opt_metric: True -> best model has high value for performance metric; \n",
    "                                    False -> best model has low value\n",
    "        :param trainsizes: A list of training set sizes (in terms of number of folds\n",
    "        :param rotation_skip: Number of CV rotations for every one rotation to train & evaluate.  \n",
    "                                Typical is 1 (train and evaluate all rotations), but when we are \n",
    "                                debugging, it is helpful to perform a smaller number of train/evaluate\n",
    "                                cycles\n",
    "        '''\n",
    "        # set the instance variables\n",
    "        self.model = model\n",
    "        self.paramsets = paramsets\n",
    "        self.trainsizes = trainsizes\n",
    "        self.eval_func = eval_func\n",
    "        self.opt_metric = opt_metric + '_mean'\n",
    "        self.opt_metric_scale = opt_metric_scale\n",
    "        self.maximize_opt_metric = maximize_opt_metric\n",
    "        self.rotation_skip = rotation_skip\n",
    "        \n",
    "        # Results attributes\n",
    "        # Full recording of all results for all paramsets, sizes, rotations,\n",
    "        # and metrics. This is a list of dictionaries for each paramset\n",
    "        self.results = []\n",
    "        # Validation summary report of all means and standard deviations for \n",
    "        # all metrics, for all paramsets, and sizes. This is a 3D s-by-r-by-p \n",
    "        # numpy array. Where s is the number of sizes, r the number of summary \n",
    "        # metrics +2, and p is the number of paramsets\n",
    "        self.report_by_size = None\n",
    "        # List of the indices of the best paramset for each size\n",
    "        self.best_param_inds = None\n",
    "        print(\"TRAINSIZES:\", self.trainsizes, trainsizes)\n",
    "\n",
    "    def load_checkpoint(self, fname):\n",
    "        ''' PROVIDED\n",
    "        Load a checkpoint file into self.results\n",
    "        \n",
    "        :param fname: Full name of the file to load the checkpoint from. \n",
    "        '''\n",
    "        if not os.path.exists(fname):\n",
    "            raise ValueError('File %s does not exist'%fname)\n",
    "        \n",
    "        with open(fname, 'rb') as f:\n",
    "            self.results = pkl.load(f)\n",
    "            \n",
    "    def dump_checkpoint(self, fname):\n",
    "        ''' PROVIDED\n",
    "        Write the current set of results to a checkpoint file\n",
    "        \n",
    "        :param fname: Full name of file to write checkpoint to\n",
    "        '''\n",
    "        with open(fname, 'wb') as f:\n",
    "            pkl.dump(self.results, f)\n",
    "            \n",
    "    def reset_results(self):\n",
    "        ''' PROVIDED\n",
    "        Reset the current set of results that are stored internally\n",
    "        '''\n",
    "        self.results = []\n",
    "    \n",
    "    def cross_validation_gridsearch(self, X, y, folds_idx, checkpoint_fname=None):\n",
    "        ''' PROVIDED\n",
    "        Perform the grid search with the given data (X, y, folds_idx).  This grid search is\n",
    "        smart in that if a specific set of hyper-parameters have already been done (encoded in\n",
    "        checkpoing), then building and evaluating for those hyper-parameters will not be done \n",
    "        again.\n",
    "        \n",
    "        :param X: Full set of input features\n",
    "        :param y: Full set of desired outputs\n",
    "        :param folds_idx: List of row indices in X/y, one for each fold\n",
    "        :param checkpoint_fname: Name of the output checkpoint file.  If None, then not written\n",
    "                to file.\n",
    "        '''\n",
    "\n",
    "        cross_val = KFoldHolisticCrossValidation(\n",
    "            self.model, self.eval_func, \n",
    "            rotation_skip = self.rotation_skip\n",
    "        )        \n",
    "\n",
    "        # Iterate over the parameter sets\n",
    "        for params in self.paramsets:\n",
    "\n",
    "            # Check that we haven't already done this before (from our checkpoint)\n",
    "            if params in [r['params'] for r in self.results]:\n",
    "                print('already evaled:', params)\n",
    "                continue\n",
    "      \n",
    "            print('evaling on:', params)\n",
    "        \n",
    "            # Set up the results for these parametrs\n",
    "            param_results = []\n",
    "            param_summary = None\n",
    "\n",
    "            # Set the parameters in the model\n",
    "            self.model.set_params(**params)\n",
    "            \n",
    "            # Iterate over the different train set sizes\n",
    "            # Running cross-validation on thm\n",
    "            for size in self.trainsizes:\n",
    "                # Perform Cross-Validation\n",
    "                result, summary = cross_val.perform_cross_validation(\n",
    "                    X, y, folds_idx, size\n",
    "                )\n",
    "                # Append results in param_results\n",
    "                param_results.append(result)\n",
    "                \n",
    "                # Append the mean and standard deviation statistics (summary)\n",
    "                if param_summary is None: \n",
    "                    param_summary = summary\n",
    "                else:\n",
    "                    # For each metric measured, append the summary results\n",
    "                    for metric in summary['train'].keys():\n",
    "                        for stat_set in ['train', 'val', 'test']:\n",
    "                            param_summary[stat_set][metric] = np.append(\n",
    "                                param_summary[stat_set][metric], \n",
    "                                summary[stat_set][metric], \n",
    "                                axis=0\n",
    "                            )\n",
    "                            \n",
    "            # Add this param's results to this accumulating results\n",
    "            self.results.append({\n",
    "                'params' :params,\n",
    "                'results':param_results, \n",
    "                'summary':param_summary\n",
    "            })\n",
    "\n",
    "            # Write the checkpoint file\n",
    "            if checkpoint_fname is not None:\n",
    "                self.dump_checkpoint(checkpoint_fname)\n",
    "        \n",
    "\n",
    "    def get_reports_all(self):\n",
    "        ''' PROVIDED\n",
    "        Generate reports on the internally stored results\n",
    "        \n",
    "        :return: Dictionary containing two keys: 'report_by_size', 'best_param_inds'\n",
    "        '''\n",
    "        self.report_by_size = self.get_reports()\n",
    "        self.best_param_inds = self.get_best_params(\n",
    "            self.opt_metric, self.maximize_opt_metric\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'report_by_size' : self.report_by_size, \n",
    "            'best_param_inds': self.best_param_inds\n",
    "        }\n",
    "    \n",
    "    # Report generation code. Provided, but you should still read it\n",
    "    \"\"\" PROVIDED\n",
    "    Functions to generate a report of the result of the cross-validation\n",
    "    \"\"\"\n",
    "    def get_reports(self):\n",
    "        ''' PROVIDED\n",
    "        Get the mean validation summary of all the parameters for each size\n",
    "        for all metrics. This is used to determine the best parameter set  \n",
    "        for each size\n",
    "        \n",
    "        RETURNS: the report_by_size as a 3D s-by-r-by-p array. Where s is \n",
    "                 the number of train sizes tried, r is the number of summary  \n",
    "                 metrics evaluated+2, and p is the number of parameter sets.\n",
    "        '''\n",
    "        results = self.results\n",
    "        sizes = np.reshape(self.trainsizes, (1, -1))\n",
    "        nsizes = sizes.shape[1]\n",
    "        nparams = len(results)\n",
    "        \n",
    "        # Set up the reports objects\n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics \n",
    "        report_by_size = np.empty((nsizes, len(colnames), nparams), dtype=object)\n",
    "\n",
    "        # Determine mean val for each paramset for each size for all metrics\n",
    "        for p, paramset_result in enumerate(results):\n",
    "            params = paramset_result['params']\n",
    "            res_val = paramset_result['summary']['val']\n",
    "\n",
    "            # Compute mean val result for each train size for each metric\n",
    "            means_by_size = [np.mean(res_val[metric], axis=1) \n",
    "                             for metric in metrics]\n",
    "            \n",
    "            print(\"MEAN\", means_by_size, len(means_by_size))\n",
    "            print(\"SIZES\", sizes, len(sizes))\n",
    "            # Include the train set sizes into the report\n",
    "            means_by_size = np.append(sizes, means_by_size, axis=0)\n",
    "            print(\"SIZES\", sizes)\n",
    "            print(\"MEAN2\", means_by_size)\n",
    "            # Include the parameter sets into the report\n",
    "            param_strgs = np.reshape([str(params)]*nsizes, (1, -1))\n",
    "            means_by_size = np.append(param_strgs, means_by_size, axis=0).T\n",
    "            print(\"MEAN3\", means_by_size)\n",
    "            # Append the parameter set means into the report \n",
    "            report_by_size[:,:,p] = means_by_size\n",
    "        return report_by_size\n",
    "\n",
    "    def get_best_params(self, opt_metric, maximize_opt_metric):\n",
    "        ''' PROVIDED (Do read through all the provided code)\n",
    "        Determines the best parameter set for each train size,  \n",
    "        based on a specific metric.\n",
    "        \n",
    "        PARAMS:\n",
    "            opt_metric: optimized metric. one of the metrics returned \n",
    "                        from eval_func, with '_mean' appended for the\n",
    "                        summary stat. This is the mean metric used to  \n",
    "                        determine the best parameter set for each size\n",
    "                        \n",
    "            maximize_opt_metric: True if the max of opt_metric should be\n",
    "                                 used to determine the best parameters.\n",
    "                                 False if the min should be used.\n",
    "        RETURNS: list of best parameter set indicies for each size \n",
    "        '''\n",
    "        results = self.results\n",
    "        report_by_size = self.report_by_size \n",
    "                \n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        \n",
    "        # Determine best params for each size, for the optimized metric\n",
    "        best_param_inds = None\n",
    "        metric_idx = metrics.index(opt_metric)\n",
    "        \n",
    "        # Report info for all paramsets for the optimized metric\n",
    "        report_opt_metric = report_by_size[:, metric_idx+2, :]\n",
    "        \n",
    "        if maximize_opt_metric:\n",
    "            # Add two for the additional cols for params and size\n",
    "            best_param_inds = np.argmax(report_opt_metric, axis=1)\n",
    "        else: \n",
    "            best_param_inds = np.argmin(report_opt_metric, axis=1)\n",
    "        # Return list of best params indices for each size\n",
    "        return best_param_inds\n",
    "    \n",
    "    def get_best_params_strings(self):\n",
    "        ''' PROVIDED\n",
    "        Generates a list of strings of the best params for each size\n",
    "        RETURNS: list of strings of the best params for each size\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        results = self.results\n",
    "        return [str(results[p]['params']) for p in best_param_inds]\n",
    "\n",
    "    def get_report_best_params_for_size(self, size):\n",
    "        ''' PROVIDED\n",
    "        Get the mean validation summary for the best parameter set \n",
    "        for a specific size for all metrics.\n",
    "        PARAMS:\n",
    "            size: index of desired train set size for the best  \n",
    "                  paramset to come from. Size here is the index in \n",
    "                  the trainsizes list, NOT the actual number of folds.\n",
    "        RETURNS: the best parameter report for the size as an s-by-m  \n",
    "                 dataframe. Where each row is for a different size, and \n",
    "                 each column is for a different summary metric.\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        report_by_size = self.report_by_size \n",
    "        \n",
    "        bp_index = best_param_inds[size]\n",
    "        size_len = len(size) if type(size) is list else 1\n",
    "                \n",
    "        metrics = list(self.results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics\n",
    "        report_best_params_for_size = pd.DataFrame(\n",
    "            report_by_size[size_idx].T[bp_index].reshape(size_len,-1),\n",
    "            columns=colnames\n",
    "        )\n",
    "        return report_best_params_for_size\n",
    "\n",
    "    \"\"\" PROVIDED\n",
    "    Plotting code to display the result of the grid search and cross-validation\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_cv(self, foldsindices, results, summary, metrics, size, metric_scales=None):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after perform_cross_validation(), \n",
    "        displaying the train and val set performances for each rotation \n",
    "        of the training set. \n",
    "        \n",
    "        PARAMS:\n",
    "            foldsindices: indices of the train sets tried\n",
    "            results: results from perform_cross_validation()\n",
    "            summary: mean and standard deviations of the results\n",
    "            metrics: list of result metrics to plot. Available metrics \n",
    "                     are the keys in the dict returned by eval_func\n",
    "            size: train set size\n",
    "            metric_scales: vector of scale factors for the metrics \n",
    "                (must be one per metric)\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        nmetrics = len(metrics)\n",
    "        if metric_scales is None:\n",
    "            metric_scales = [1] * nmetrics\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax, scale in zip(metrics, axs, metric_scales):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(results['train'][metric], axis=1)\n",
    "            res_val = np.mean(results['val'][metric], axis=1)\n",
    "            #res_test = np.mean(results['test'][metric], axis=1)\n",
    "            # Plot\n",
    "            ax.plot(foldsindices, scale*res_train, label='train')\n",
    "            ax.plot(foldsindices, scale*res_val, label='val')\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[0].legend(loc='upper right')\n",
    "        axs[0].set(xlabel='Fold Index')\n",
    "        axs[0].set(title='Performance for Train Set Size ' + str(size))\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_param_train_val(self, metrics, paramidx=0, view_test=False, metric_scales=None):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), \n",
    "        displaying the mean (summary) train and val set performances \n",
    "        for each train set size.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std'\n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "            paramidx: parameter set index\n",
    "            view_test: flag to view the test set results\n",
    "            metric_scales: vector of scale factors for the metrics \n",
    "                (must be one per metric)\n",
    "                \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "\n",
    "        summary = results[paramidx]['summary']\n",
    "        params = results[paramidx]['params']\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "        if metric_scales is None:\n",
    "            metric_scales = [1] * nmetrics\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        #fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax, scale in zip(metrics, axs, metric_scales):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(summary['train'][metric], axis=1)\n",
    "            res_val = np.mean(summary['val'][metric], axis=1)\n",
    "            # Plot\n",
    "            ax.plot(sizes, scale*res_train, label='train')\n",
    "            ax.plot(sizes, scale*res_val, label='val')\n",
    "            if view_test:\n",
    "                res_test = np.mean(summary['test'][metric], axis=1)\n",
    "                ax.plot(sizes, res_test, label='test')\n",
    "            ax.set(ylabel=metric)\n",
    "            ax.set_xticks(sizes)\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title=str(params))\n",
    "        axs[0].legend(loc='upper right')\n",
    "        return fig, axs\n",
    "    \n",
    "    def plot_allparams_val(self, metrics, metric_scales=None):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), displaying  \n",
    "        mean (summary) validation set performances for each train size \n",
    "        for all parameter sets for the specified metrics.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std' \n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "            metric_scales: vector of scale factors for the metrics \n",
    "                (must be one per metric)\n",
    "                \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "        if metric_scales is None:\n",
    "            metric_scales = [1] * nmetrics\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(10,6))\n",
    "        \n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax, scale in zip(metrics, axs, metric_scales):\n",
    "            for p, param_results in enumerate(results):\n",
    "                summary = param_results['summary']\n",
    "                params = param_results['params']\n",
    "                # Compute the mean for multiple outputs\n",
    "                res_val = np.mean(summary['val'][metric], axis=1)                \n",
    "                ax.plot(sizes, scale*res_val, label=str(params))\n",
    "            ax.set(ylabel=metric)\n",
    "            ax.set_xticks(sizes)\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title='Validation Performance')\n",
    "        axs[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_best_params_by_size(self):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), displaying \n",
    "        mean (summary) train and validation set performances for the best \n",
    "        parameter set for each train size for the optimized metric.\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        results = self.results\n",
    "        metric = self.opt_metric\n",
    "        best_param_inds = self.best_param_inds\n",
    "        sizes = np.array(self.trainsizes)\n",
    "\n",
    "        # Unique set of best params for the legend\n",
    "        unique_param_sets = np.unique(best_param_inds)\n",
    "        lgnd_params = [self.paramsets[p] for p in unique_param_sets]\n",
    "\n",
    "        # Initialize figure\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(10,6))\n",
    "        #fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "        set_names = ['train', 'val']\n",
    "\n",
    "        # Construct each subplot\n",
    "        for i, (ax, set_name) in enumerate(zip(axs, set_names)):\n",
    "            for p in unique_param_sets:\n",
    "                # Obtain indices of sizes this paramset was best for\n",
    "                param_size_inds = np.where(best_param_inds == p)[0]\n",
    "                param_sizes = sizes[param_size_inds]\n",
    "                param_sizes.sort()\n",
    "                # Compute the mean over multiple outputs for each size\n",
    "                param_summary = results[p]['summary'][set_name]\n",
    "                metric_scores = np.mean(param_summary[metric][param_size_inds,:], axis=1)\n",
    "                # Plot the param results for each size it was the best for\n",
    "                ax.scatter(param_sizes, self.opt_metric_scale * metric_scores, s=120, marker=(p+2, 1))\n",
    "                ax.set_xticks(param_sizes)\n",
    "                #ax.grid(True)\n",
    "\n",
    "            set_name += ' Set Performance'\n",
    "            ax.set(ylabel=metric, title=set_name)\n",
    "\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].legend(lgnd_params, bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        return fig, axs\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPivOSSzA_yZ",
    "tags": []
   },
   "source": [
    "# PERFORM CROSS VALIDATION\n",
    "Initialize holistic cross validation objects to explore Linear, Ridge, Lasso, and ElasticNet models.\n",
    "\n",
    "The inputs for the models are the MI data and the outputs are the torque (you'll provide the shoulder and elbow simulataneouly, as done in the previous HW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl8vO7yOA_yZ"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Holistic Cross Validation Options:\n",
    "* ridge_alphas: list of alphas to try for the RIDGE model\n",
    "* lasso_alphas: list of alphas to try for the LASSO model\n",
    "* en_alphas: list of alphas to try for the ELASTICNET model\n",
    "* l1_ratios: list of l1_ratios to try for the ELASTICNET model\n",
    "\n",
    "* trainsizes: list of number of folds to utilize in the train set\n",
    "* opt_metric: the optimized metric, returned by the eval_func, used \n",
    "  to select the best parameter sets\n",
    "* maximize_opt_metric: True if the opt_metric is maximized; False \n",
    "  otherwise\n",
    "* skip: the number of folds to skip when rotating through train sets \n",
    "  of the same size\n",
    "\"\"\"\n",
    "ridge_alphas = [1, 10, 100, 1000, 10000] \n",
    "lasso_alphas = [.00001, .0001, .001, .01, .1] \n",
    "en_alphas = [.0001, .001, .01, .1, 0.2, 0.4] \n",
    "l1_ratios = [.001, .01, .1, 0.9] \n",
    "\n",
    "trainsizes = [1, 2, 3, 5, 8, 13, 18]  #range(1, nfolds-1)\n",
    "opt_metric = 'rmse'\n",
    "maximize_opt_metric = False\n",
    "# TODO: set skip to 1 for your final results\n",
    "skip = 4\n",
    "\n",
    "# True to always run cross validation, false to re-load existing run\n",
    "# or run cross validation for the first time\n",
    "force = False \n",
    "\n",
    "# Tag for the filename to save the experiments to\n",
    "#prefix = \"/content/drive/MyDrive/Colab Notebooks/hw7_skip%d\"%(skip) \n",
    "prefix = \"./hw7_skip%d\"%(skip) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upCTWszGA_ya",
    "tags": []
   },
   "source": [
    "## [LINEAR REGRESSION](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "Ordinary least squares Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3_Zpg-LA_ya",
    "outputId": "cdf2ef35-fd55-4de9-9677-ed057f8794ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "LinearRegression\n",
    "\n",
    "Execute cross validation procedure for all sizes for the \n",
    "LinearRegression model using cross_validation_gridsearch().\n",
    "The parameter list for the LinearRegression model is a\n",
    "list with just an empty dictionary [{}]\n",
    "\"\"\"\n",
    "time = timelib.time()\n",
    "# where to save the cross-validation results.\n",
    "# By default this will be in your Colab Notebook folder,\n",
    "# if you wish to save it elsewhere replace this with the full paths\n",
    "lnr_fullcvfname = prefix + \"_linear_crossval.pkl\"\n",
    "\n",
    "#lnr_checkpoint_fun = save_checkpoint_generator(lnr_fullcvfname)\n",
    "\n",
    "# TODO: create the model and cross val grid search objects\n",
    "model = LinearRegression()\n",
    "\n",
    "lnr_crossval = CrossValidationGridSearch(\n",
    "    model, [{}], score_eval, \n",
    "    opt_metric, maximize_opt_metric=maximize_opt_metric,\n",
    "    trainsizes=trainsizes, rotation_skip=skip,\n",
    "    opt_metric_scale = 180/np.pi\n",
    ")\n",
    "\n",
    "# Should we load the checkpoint file?\n",
    "if not force and os.path.exists(lnr_fullcvfname):\n",
    "    checkpoint = lnr_crossval.load_checkpoint(lnr_fullcvfname)\n",
    "\n",
    "# Use grid_cross_validation() to run the full cross \n",
    "#       validation procedure\n",
    "# Note: when testing, run this using small lists of parameters \n",
    "#       (e.g. of length 2 or 4) and/or small trainsize lists \n",
    "#       (e.g. [1, 2, 3, 4, 5])\n",
    "# Note: for the final submission, make sure to use the complete \n",
    "#       parameter set list and trainsize list provided/specified\n",
    "#       This will take some time (longer than an hour)\n",
    "    \n",
    "lnr_crossval.cross_validation_gridsearch(\n",
    "        MI, torque, folds_idx,\n",
    "        lnr_fullcvfname \n",
    "    )\n",
    "lnr_crossval_report = lnr_crossval.get_reports_all()\n",
    "    \n",
    "lnr_crossval_report.keys()\n",
    "    \n",
    "# Report \n",
    "print(\"Elapsed Time: %.2f min\" % ((timelib.time() - time) / 60))\n",
    "lnr_crossval.model, lnr_crossval.rotation_skip, lnr_crossval.trainsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNNCimpSA_yb"
   },
   "source": [
    "## [RIDGE](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n",
    "\n",
    "$\\min_w ||y - w^TX||^2_2 + \\alpha ||w||^2_2$\n",
    "\n",
    "$\\alpha$: amount of $L_2$ regularization to apply. Larger $\\alpha$ greater penalize the model for larger weights\n",
    "\n",
    "$w$: the weights from the model\n",
    "\n",
    "$X$: feature or input data\n",
    "\n",
    "$y$: true outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjtxMxkqA_yc",
    "outputId": "650736a0-61b7-49b1-f6a4-913edffd0f63",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE\n",
    "\n",
    "Initialize a GridSearchCrossValidation object that uses RIDGE\n",
    "as the model, and the provided r_allparamsets\n",
    "\n",
    "Execute cross validation procedure for all sizes for the Ridge\n",
    "model using cross_validation_gridsearch()\n",
    "\"\"\"\n",
    "time = timelib.time()\n",
    "r_fullcvfname = prefix + \"_ridge_crossval.pkl\"\n",
    "\n",
    "r_param_lists = {'alpha':ridge_alphas}\n",
    "r_allparamsets = generate_paramsets(r_param_lists)\n",
    "#print(pd.DataFrame(r_allparamsets))\n",
    "\n",
    "# TODO: Initialize a GridSearchCrossValidation object using Ridge\n",
    "model = Ridge()\n",
    "r_crossval = # TODO\n",
    "\n",
    "# Report\n",
    "print(\"Elapsed Time: %.2f min\" % ((timelib.time() - time) / 60))\n",
    "r_crossval.model, r_crossval.rotation_skip, r_crossval.trainsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6IHM3CIA_yc"
   },
   "source": [
    "## [LASSO](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "\n",
    "$\\min_w \\frac{1}{2 N} ||y - w^TX||^2_2 + \\alpha ||w||_1$\n",
    "\n",
    "$N$: the number of samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xCwVxod1A_yd",
    "outputId": "6ad7dce3-ae50-4175-f5f6-d0c4bbfb8687",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO\n",
    "\n",
    "Initialize a GridSearchCrossValidation object that uses LASSO\n",
    "as the model, and the provided l_allparamsets\n",
    "\n",
    "Execute cross validation procedure for all sizes for the Lasso\n",
    "model using cross_validation_gridsearch()\n",
    "\"\"\"\n",
    "time = timelib.time()\n",
    "\n",
    "l_fullcvfname = prefix + \"_lasso_crossval.pkl\"\n",
    "\n",
    "l_param_lists = {'alpha':lasso_alphas, 'max_iter':[int(1e4)]}\n",
    "l_allparamsets = #TODO\n",
    "\n",
    "# Report\n",
    "print(\"Elapsed Time: %.2f min\" % ((timelib.time() -time) / 60))\n",
    "l_crossval.model, l_crossval.rotation_skip, l_crossval.trainsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5Gjy0BMA_ye"
   },
   "source": [
    "## [ELASTICNET](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)\n",
    "\n",
    "$\\min_w \\frac{1}{2 N} ||y - w^TX||^2_2 + \\alpha L_1 ||w||_1 + \\frac{1}{2} \\alpha (1 - L_1) ||w||^2_2$\n",
    "\n",
    "$\\alpha$: Regularization parameter\n",
    "\n",
    "$L_1$: the $L_1$ ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqRcGE1XA_ye",
    "outputId": "1818bcbd-a5ce-4d15-cf0d-13e6f631390f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET\n",
    "\n",
    "Initialize a GridSearchCrossValidation object that uses ELASTICNET\n",
    "as the model, and the provided allparamsets\n",
    "\n",
    "Execute cross validation procedure for all sizes for the ELASTICNET\n",
    "model using cross_validation_gridsearch)\n",
    "\n",
    "Re-load the existing experiment\n",
    "\"\"\"\n",
    "time = timelib.time()\n",
    "en_fullcvfname = prefix + \"_crossval.pkl\"\n",
    "\n",
    "en_param_lists = {'alpha':en_alphas, 'l1_ratio':l1_ratios, 'max_iter':[int(1e4)]}\n",
    "en_allparamsets = generate_paramsets(en_param_lists)\n",
    "\n",
    "model = ElasticNet()\n",
    "en_crossval =  #TODO\n",
    "# Report\n",
    "print(\"Elapsed Time: %.2f min\" % ((timelib.time() - time) / 60))\n",
    "\n",
    "crossval.model, crossval.rotation_skip, crossval.trainsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYpBzG57A_yf"
   },
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh2c8zB_A_yf"
   },
   "source": [
    "### Explore the result output structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbYpF0vnA_yf",
    "outputId": "77639bb4-c50a-49e9-e6d5-8ee2e207514e"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "List GridSearchCrossValidation Attributes\n",
    "\"\"\"\n",
    "dir(en_crossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cex03-XHA_yf",
    "outputId": "d6eb217f-e3b3-43fb-e934-835cece9de0f"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Results attribute is a list of dictionaries. Each element, or dictionary\n",
    "corresponds to the results for a single parameter set\n",
    "\"\"\"\n",
    "len(en_crossval.results), en_crossval.results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieqVYCHmA_yg",
    "outputId": "a0442727-31cc-446b-d698-06dbec136c99"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "* crossval.results[0]['results'] is a list of dictionaries with the results\n",
    "  for each size for the parameter set at index 0\n",
    "* crossval.results[1]['summary'] is a dictionary of summary results for the \n",
    "  train, val, and test sets for the parameter set at index 1\n",
    "\"\"\"\n",
    "len(en_crossval.results[0]['results']), en_crossval.results[1]['summary'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqd7SuxDA_yg",
    "outputId": "acdcb386-ec25-443e-bafe-11ff67ee0503"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "* crossval.results[0]['results'][2] is a dictionary with the results\n",
    "  for the train size at index 2 for the parameter set at index 0\n",
    "* crossval.results[1]['summary']['val'] is a dictionary of summary (over the \n",
    "  sizes) results for the val set for the parameter set at index 1, for all \n",
    "  metrics\n",
    "\"\"\"\n",
    "en_crossval.results[0]['results'][2].keys(), en_crossval.results[1]['summary']['val'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWiBLCSdA_yh",
    "outputId": "72647f5e-1095-450a-e558-4e6cf96bb1a3"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "* crossval.results[0]['results'][2]['train'] is a dictionary of all results \n",
    "  for the train set for the parameter set at index 0, the size at index 2, for \n",
    "  all metrics\n",
    "* crossval.results[1]['summary']['val']['mse_mean'] is a numpy array of \n",
    "  averages for the val set for the parameter set at index 1, for the mse. The\n",
    "  averages are computed over the sizes\n",
    "\"\"\"\n",
    "en_crossval.results[0]['results'][2]['train'].keys(), en_crossval.results[1]['summary']['val']['mse_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJijJpMjA_yh",
    "outputId": "4bf6a35a-5215-49cd-e291-98dc9dc61e8b"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "* crossval.results[0]['results'][2]['train']['mse'] is a dictionary of all \n",
    "  results for the train set for the parameter set at index 0, the size at \n",
    "  index 2, for the mse, for all rotations (there are 20 rotations when skip=1)\n",
    "\"\"\"\n",
    "en_crossval.results[0]['results'][2]['train']['mse'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq9rc3NkA_yi"
   },
   "source": [
    "### Best Parameters for Each Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRRr6ks5A_yi"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Results options:\n",
    "* size_idx: index of the size from the list of train sizes to examine results\n",
    "* metrics: list of summary (average) metrics to examine results\n",
    "\"\"\"\n",
    "# index 3 corresponds to train size 5\n",
    "size_idx = 3\n",
    "metrics = ['rmse_mean', 'fvaf_mean']\n",
    "metric_scales = [180/np.pi, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGiFXBpyA_yj",
    "outputId": "4c8647e5-0c00-42e5-b279-7db6e0f9e242"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display the lists of the best parameter sets for each size for all\n",
    "the models, expect the Linear model (as it has only one parameter set)\n",
    "\"\"\"\n",
    "print(\"Best Parameter Sets For Each Train Set Size\")\n",
    "\n",
    "print(\"RIDGE\")\n",
    "r_best_param_info = pd.DataFrame(\n",
    "    (\n",
    "        r_crossval.trainsizes, \n",
    "        r_crossval.best_param_inds, \n",
    "        r_crossval.get_best_params_strings()\n",
    "    ),\n",
    "    index=['train_size','param_index','paramset']\n",
    ")\n",
    "print(r_best_param_info.T)\n",
    "\n",
    "\n",
    "print(\"\\nLASSO\")\n",
    "l_best_param_info = pd.DataFrame(\n",
    "    (\n",
    "        l_crossval.trainsizes, \n",
    "        l_crossval.best_param_inds, \n",
    "        l_crossval.get_best_params_strings()\n",
    "    ),\n",
    "    index=['train_size','param_index','paramset']\n",
    ")\n",
    "print(l_best_param_info.T)\n",
    "\n",
    "print(\"\\nELASTICNET\")\n",
    "best_param_info = pd.DataFrame(\n",
    "    (\n",
    "        en_crossval.trainsizes, \n",
    "        en_crossval.best_param_inds, \n",
    "        en_crossval.get_best_params_strings()\n",
    "    ),\n",
    "    index=['train_size', 'param_index', 'paramset']\n",
    ")\n",
    "print(best_param_info.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UrsX7ctA_yj"
   },
   "source": [
    "### Plot Best Parameters for Each Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "SqB1PBHYA_yj",
    "outputId": "9fdfad05-f492-46fc-86ee-043bd994f615"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "LINEAR REGRESSION\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "each train size for the optimized metric. Use plot_best_params_by_size()\n",
    "\n",
    "Note: for LinearRegression, there is only one parameter set.\n",
    "\"\"\"\n",
    "lnr_crossval.plot_best_params_by_size() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "DPHPt712A_yj",
    "outputId": "ea3147f6-7e6c-40b9-94c9-e2458a1e0789"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "FOj1zJQ6A_yk",
    "outputId": "747ee431-e9d0-4ac9-98cc-088124633efc"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "zEWcPCR1A_yl",
    "outputId": "9ef9cf57-9184-47dc-cec2-c882f6df454b"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acmKQcEcA_yp"
   },
   "source": [
    "### Plot Performance over the Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3r3YcsRA_yr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDED\n",
    "'''\n",
    "def plot_param_val_for_size(crossval, metric, alphas, sizeidx=0, \n",
    "                            metric_scale=1, xlog=False):\n",
    "    ''' \n",
    "    Plotting function for after cross_validation_gridsearch(), \n",
    "    displaying the mean (summary) train and val set performances \n",
    "    for each alpha, given the size, for RIDGE and LASSO only\n",
    "\n",
    "    PARAMS:\n",
    "        crossval: cross validation object\n",
    "        metric: summary metric to plot. '_mean' or '_std' must be \n",
    "                append to the end of the base metric name. These \n",
    "                base metric names are the keys in the dict returned\n",
    "                by eval_func\n",
    "        alphas: list of alpha values\n",
    "        sizeidx: train size index\n",
    "        metric_scale: scale to multiply the metric by before plotting\n",
    "        xlog: Use log scale along the x axis\n",
    "\n",
    "    RETURNS: the figure and axes handles\n",
    "    '''\n",
    "    sizes = crossval.trainsizes\n",
    "    results = crossval.results\n",
    "    best_param_inds = crossval.best_param_inds\n",
    "\n",
    "    nalphas = len(alphas)\n",
    "\n",
    "    nsizes = len(sizes)\n",
    "\n",
    "    # Initialize the matrices for the curve\n",
    "    Y_train = np.empty((nalphas,))\n",
    "    Y_val = np.empty((nalphas,))\n",
    "\n",
    "    # Obtain the mean performance for the curve\n",
    "    for param_res in results:\n",
    "        params = param_res['params']\n",
    "        summary = param_res['summary']\n",
    "\n",
    "        alpha_idx = alphas.index(params['alpha'])\n",
    "\n",
    "        # Compute the mean for multiple outputs\n",
    "        res_train = np.mean(summary['train'][metric][sizeidx, :])\n",
    "        Y_train[alpha_idx] = res_train\n",
    "\n",
    "        res_val = np.mean(summary['val'][metric][sizeidx, :])\n",
    "        Y_val[alpha_idx] = res_val\n",
    "    \n",
    "    # Initialize figure plots\n",
    "    fig = plt.figure(figsize=(12,2))\n",
    "    for i, (Y, set_name) in enumerate(zip((Y_train, Y_val), \n",
    "                                          ('Training', 'Validation'))):\n",
    "        # Plot\n",
    "        ax = fig.add_subplot(1, 2, i+1)\n",
    "        print(alphas)\n",
    "        ax.plot(alphas, metric_scale * Y)\n",
    "        ax.set_xticks(alphas)\n",
    "        title = \"%s Performance, Train Size %d Folds\" % (set_name, \n",
    "                                                         sizes[sizeidx])\n",
    "        ax.set(title=title)\n",
    "        ax.set(xlabel=\"alpha\", ylabel=metric)\n",
    "        if xlog:\n",
    "            ax.set_xscale('log')\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QmB4IsOA_yr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDED\n",
    "'''\n",
    "def plot_surface(xlist, ylist, Z_train, Z_val, \n",
    "                 xlabel, ylabel, zlabel, \n",
    "                 elev=30, angle=45, title_suffix=\"\",\n",
    "                xticks=None, yticks=None,\n",
    "                xlog=False, ylog=False,\n",
    "                figsize=(10,5),\n",
    "                tick_decimals=None):\n",
    "    ''' \n",
    "    Helper plotting function. x-axis is always alpha\n",
    "    \n",
    "    REQUIRES: from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    PARAMS:\n",
    "        xlist: list of x values\n",
    "        ylist: list of y values\n",
    "        Z_train: matrix of performance results from the training set\n",
    "        Z_val: matrix of performance results from the validation set\n",
    "        ylabel: y-axis label \n",
    "        zlabel: z-axis label\n",
    "        elev: elevation of the 3D plot for the view\n",
    "        angle: angle in degrees of the 3D plot for the view\n",
    "        title_suffix: string to append to each subplot title\n",
    "        xticks: specify x tick locations\n",
    "        yticks: specify y tick locations\n",
    "        xlog: Use log scale for x axis\n",
    "        ylog: Use log scale for y axis\n",
    "        figsize: Size of the figure\n",
    "        tick_decimals: If specified, number of digits after the decimal point\n",
    "\n",
    "    RETURNS: the figure\n",
    "    '''\n",
    "    # Initialize figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create the X/Y coordinates for the grid\n",
    "    X, Y = np.meshgrid(xlist, ylist) \n",
    "    \n",
    "    # Use log of X in plot ?\n",
    "    if xlog:\n",
    "        X = np.log10(X)\n",
    "        xticks = np.log10(xticks)\n",
    "        \n",
    "    # Use log of Y in plot ?\n",
    "    if ylog:\n",
    "        Y = np.log10(Y)\n",
    "        yticks = np.log10(yticks)\n",
    "        \n",
    "    # Plot Training and Validation performance\n",
    "    for i, (Z, set_name) in enumerate(zip((Z_train, Z_val), \n",
    "                                          ('Training', 'Validation'), \n",
    "                                              )):\n",
    "        # Plot the surface\n",
    "        ax = fig.add_subplot(1,2, i+1, projection='3d')\n",
    "        surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, \n",
    "                               linewidth=0, antialiased=False)\n",
    "        title = \"%s Performance %s\" % (set_name, title_suffix)\n",
    "        ax.view_init(elev=elev, azim=angle)\n",
    "        ax.set(title=title)\n",
    "        ax.set(xlabel=xlabel, ylabel=ylabel, zlabel=zlabel)\n",
    "        \n",
    "        # Set tick marks\n",
    "        if xticks is not None:\n",
    "            tmp = xticks\n",
    "            \n",
    "            # Round the ticks if requested\n",
    "            if tick_decimals is not None:\n",
    "                tmp = np.round(xticks, decimals=tick_decimals)\n",
    "            ax.set_xticks(tmp)\n",
    "            \n",
    "        # Round the ticks if requested\n",
    "        if yticks is not None:\n",
    "            tmp = yticks\n",
    "            if tick_decimals is not None:\n",
    "                tmp = np.round(yticks, decimals=tick_decimals)\n",
    "            ax.set_yticks(tmp)\n",
    "            \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqfJDbtpA_yr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDED\n",
    "'''\n",
    "def plot_param_val_surface_RL(crossval, metric, alphas, metric_scale=1,\n",
    "                              elev=30, angle=245):\n",
    "    ''' \n",
    "    Plotting function for after cross_validation_gridsearch(), \n",
    "    displaying the mean (summary) train and val set performances \n",
    "    for each alpha, for all sizes, for RIDGE and LASSO only\n",
    "    \n",
    "    REQUIRES: from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    PARAMS:\n",
    "        crossval: cross validation object\n",
    "        metric: summary metric to plot. '_mean' or '_std' must be \n",
    "                append to the end of the base metric name. These \n",
    "                base metric names are the keys in the dict returned\n",
    "                by eval_func\n",
    "        alphas: list of alpha values\n",
    "        elev: elevation of the 3D plot for the view\n",
    "        angle: angle in degrees of the 3D plot for the view\n",
    "\n",
    "    RETURNS: the figure and axes handles\n",
    "    '''\n",
    "    sizes = crossval.trainsizes\n",
    "    results = crossval.results\n",
    "    best_param_inds = crossval.best_param_inds\n",
    "    nalphas = len(alphas)\n",
    "\n",
    "    nsizes = len(sizes)\n",
    "\n",
    "    # Initialize the matrices for the surface\n",
    "    Z_train = np.empty((nsizes, nalphas))\n",
    "    Z_val = np.empty((nsizes, nalphas))\n",
    "\n",
    "    # Obtain the mean performance for the surface\n",
    "    for param_res in results:\n",
    "        params = param_res['params']\n",
    "        summary = param_res['summary']\n",
    "\n",
    "        alpha_idx = alphas.index(params['alpha'])\n",
    "\n",
    "        # Compute the mean for multiple outputs\n",
    "        res_train = np.mean(summary['train'][metric], axis=1)\n",
    "        Z_train[:, alpha_idx] = res_train\n",
    "\n",
    "        # Compute the mean for multiple outputs\n",
    "        res_val = np.mean(summary['val'][metric], axis=1)\n",
    "        Z_val[:, alpha_idx] = res_val\n",
    "    \n",
    "    fig = plot_surface(alphas, \n",
    "                       sizes, \n",
    "                       Z_train*metric_scale, \n",
    "                       Z_val*metric_scale, \n",
    "                       'log alpha',\n",
    "                       'size (# of folds)', \n",
    "                       metric, elev, angle,\n",
    "                      xticks=alphas, \n",
    "                       yticks=sizes,\n",
    "                      xlog=True,\n",
    "                      tick_decimals=1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEWlOxHKA_ys"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDED\n",
    "'''\n",
    "def plot_param_val_surface_EN(crossval, metric, param_lists, \n",
    "                              metric_scale=1,\n",
    "                              sizeidx=0, elev=35, angle=280):\n",
    "    ''' \n",
    "    Plotting function for after cross_validation_gridsearch(), \n",
    "    displaying the mean (summary) train and val set performances \n",
    "    for each alpha and l1_ratio, given the size, for the ELASTICNET\n",
    "    \n",
    "    REQUIRES: from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    PARAMS:\n",
    "        crossval: cross validation object\n",
    "        metric: summary metric to plot. '_mean' or '_std' must be \n",
    "                append to the end of the base metric name. These \n",
    "                base metric names are the keys in the dict returned\n",
    "                by eval_func\n",
    "        param_lists: dictionary of the list of alphas and l1_ratios\n",
    "        sizeidx: train size index\n",
    "        elev: elevation of the 3D plot for the view\n",
    "        angle: angle in degrees of the 3D plot for the view\n",
    "        metric_scale: scale factor to apply to metric before plotting\n",
    "\n",
    "    RETURNS: the figure \n",
    "    '''\n",
    "    sizes = crossval.trainsizes\n",
    "    results = crossval.results\n",
    "    best_param_inds = crossval.best_param_inds\n",
    "\n",
    "    alphas = list(param_lists['alpha'])\n",
    "    l1_ratios = list(param_lists['l1_ratio'])\n",
    "\n",
    "    nalphas = len(alphas)\n",
    "    nl1_ratios = len(l1_ratios)\n",
    "\n",
    "    nsizes = len(sizes)\n",
    "\n",
    "    # Initialize the matrices for the surface\n",
    "    Z_train = np.empty((nl1_ratios, nalphas))\n",
    "    Z_val = np.empty((nl1_ratios, nalphas))\n",
    "\n",
    "    # Obtain the mean performance for the surface \n",
    "    for param_res in results:\n",
    "        params = param_res['params']\n",
    "        summary = param_res['summary']\n",
    "\n",
    "        alpha_idx = alphas.index(params['alpha'])\n",
    "        l1_idx = l1_ratios.index(params['l1_ratio'])\n",
    "\n",
    "        # Compute the mean for multiple outputs\n",
    "        res_train = np.mean(summary['train'][metric][sizeidx, :])\n",
    "        Z_train[l1_idx, alpha_idx] = res_train\n",
    "\n",
    "        res_val = np.mean(summary['val'][metric][sizeidx, :])\n",
    "        Z_val[l1_idx, alpha_idx] = res_val\n",
    "    \n",
    "    fig = plot_surface(alphas, l1_ratios, \n",
    "                       Z_train*metric_scale, \n",
    "                       Z_val*metric_scale, \n",
    "                       'log alpha',\n",
    "                       'log l1_ratio', \n",
    "                       metric, elev, angle,\n",
    "                       ', Size %d Folds'% sizes[sizeidx],\n",
    "                      xticks=alphas,\n",
    "                      yticks=l1_ratios,\n",
    "                      xlog=True,\n",
    "                      ylog=True,\n",
    "                      figsize=(8,4),\n",
    "                      tick_decimals=1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMica7c3A_ys",
    "outputId": "226ba551-4622-4d01-baf3-806b6edae7c5"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "List the parameter sets explored for RIDGE\n",
    "\"\"\"\n",
    "r_crossval.paramsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "emroPiU4A_yt",
    "outputId": "052a84fb-561f-4f5a-ab8d-203e309fc702"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "RIDGE\n",
    "Use plot_param_val_surface_RL() to plot the surface of the training\n",
    "and validation set performance versus alpha and size in the X and Y axes,\n",
    "using the optimized metric\n",
    "\"\"\"\n",
    "# Feel free to adjust these to understand the shape of the surface\n",
    "# Elevation of the plot\n",
    "elev = 30\n",
    "# Angle the plot is viewed\n",
    "angle = 255\n",
    "\n",
    "# Plot\n",
    "plot_param_val_surface_RL(r_crossval, r_crossval.opt_metric, \n",
    "                          ridge_alphas, elev=elev, angle=angle,\n",
    "                         metric_scale=r_crossval.opt_metric_scale)\n",
    "\n",
    "# UNCLEAR WHY WE ARE GETTING TWO COPIES OF EACH PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFMLvBIYA_yt",
    "outputId": "42a7fa1d-f9dd-4afb-d4af-04f45ab5a0d5"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "List the parameter sets explored for LASSO\n",
    "\"\"\"\n",
    "l_crossval.paramsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "I0DTwPNkA_yx",
    "outputId": "00fc37f0-176c-479e-c572-963472d1eff6"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO\n",
    "Use plot_param_val_surface_RL() to plot the surface of the training\n",
    "and validation set performance versus alpha and size in the X and Y axes,\n",
    "using the optimized metric\n",
    "\"\"\"\n",
    "# Feel free to adjust these to understand the shape of the surface\n",
    "# Elevation of the plot\n",
    "elev = 30\n",
    "# Angle the plot is viewed\n",
    "angle = 255\n",
    "\n",
    "# TODO: Plot\n",
    "plot_param_val_surface_RL(# TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glUcIDKIA_yz",
    "outputId": "9c899886-2b5b-4126-e459-30d5ba71e1a4"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "List the parameter sets explored for ELASTICNET\n",
    "\"\"\"\n",
    "en_crossval.paramsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875
    },
    "id": "ogrPGvvZA_y0",
    "outputId": "27ff77f1-53b8-4f64-cefe-fb8af5476e47"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "ELASTICNET\n",
    "Use plot_param_val_surface_EN() to plot the surface of the training\n",
    "and validation set performance versus alpha and l1_ratio in the X \n",
    "and Y axes for the size indices of 0, 3, and 6, for crossval.opt_metric\n",
    "\"\"\"\n",
    "# Feel free to adjust these to understand the shape of the surface\n",
    "# Elevation of the plot\n",
    "elev = 25\n",
    "# Angle the plot is viewed\n",
    "angle = 300\n",
    "\n",
    "# Plot\n",
    "for si in [0, 2, 5]:\n",
    "    plot_param_val_surface_EN(en_crossval, en_crossval.opt_metric, \n",
    "                              param_lists=en_param_lists, sizeidx=si, \n",
    "                              elev=elev, angle=angle,\n",
    "                             metric_scale=r_crossval.opt_metric_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRnAlAoPA_y1"
   },
   "source": [
    "### Paired t-tests\n",
    "We can use paired t-tests to assess statistically significant differences between the mean test set performances of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6MMOM6OA_y2"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Obtain all the results for all the models\n",
    "\"\"\"\n",
    "# LinearRegression\n",
    "lnr_all_results = lnr_crossval.results\n",
    "\n",
    "# RIDGE\n",
    "r_all_results = r_crossval.results\n",
    "\n",
    "# LASSO\n",
    "l_all_results = l_crossval.results\n",
    "\n",
    "# ELASTICNET\n",
    "all_results = en_crossval.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "id": "DahOlMCqA_y2",
    "outputId": "ec8ab7d1-5200-463d-cd44-c27b903774d5"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "\n",
    "Plot distributions of the Validation and Test scores from the\n",
    "best parameter set for each base model for the corresponding \n",
    "size indices, [0, 3, 5]. The metric of interest is rmse.\n",
    "These are the distribution of results from each rotation of \n",
    "the training set\n",
    "\"\"\"\n",
    "metric = 'rmse'\n",
    "set_names = ['val', 'test']\n",
    "scale = 180.0/np.pi\n",
    "\n",
    "# Size indices\n",
    "size_indices = [0, 3, 5]\n",
    "\n",
    "for si in size_indices:\n",
    "    # Obtain the index of the best parameter set for the size\n",
    "    # RIDGE\n",
    "    r_bp_idx = r_crossval.best_param_inds[si]\n",
    "    # LASSO\n",
    "    l_bp_idx = l_crossval.best_param_inds[si]\n",
    "    # ELASTICNET\n",
    "    bp_idx = en_crossval.best_param_inds[si]\n",
    "\n",
    "    # Construct the figure\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "    for i, set_name in enumerate(set_names):\n",
    "        title = '%s, Size %d folds' % (set_name, trainsizes[si])\n",
    "\n",
    "        # LINEAR\n",
    "        # Note: there's only 1 parameter set for the Linear model\n",
    "        lnr_res = lnr_all_results[0]['results'][si][set_name]\n",
    "        lnr_scores = np.mean(lnr_res[metric], axis=1) * scale\n",
    "\n",
    "        # RIDGE\n",
    "        # Obtain results for the best parameter set for the size\n",
    "        ridge_res = r_all_results[r_bp_idx]['results'][si][set_name]\n",
    "        # Compute the mean of the outputs for each data set rotation\n",
    "        ridge_scores = np.mean(ridge_res[metric], axis=1) * scale\n",
    "\n",
    "        # LASSO\n",
    "        lasso_res = l_all_results[l_bp_idx]['results'][si][set_name]\n",
    "        lasso_scores = np.mean(lasso_res[metric], axis=1) * scale\n",
    "        \n",
    "        # ELASTICNET\n",
    "        res = all_results[bp_idx]['results'][si][set_name]\n",
    "        elastic_scores = np.mean(res[metric], axis=1) * scale\n",
    "        \n",
    "        # Determine the edges for the bins in the histograms\n",
    "        all_scores = np.concatenate((elastic_scores, ridge_scores, \n",
    "                                     lasso_scores, lnr_scores))\n",
    "    \n",
    "        \n",
    "        # Boxplots\n",
    "        axs[i].boxplot([elastic_scores, ridge_scores, \n",
    "                           lasso_scores, lnr_scores])\n",
    "        axs[i].set_xticklabels(['ElasticNet', 'Ridge', 'Lasso', 'Linear'])\n",
    "        axs[i].set(ylabel=metric)\n",
    "        axs[i].set(title=title, xlabel=metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tg5eRtgUA_y2",
    "outputId": "8ed139ba-007a-4c33-80f2-45086ec665c2"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Paired t-test\n",
    "Two-sided t-test for the null hypothesis that mean of the distribution\n",
    "of differences between the two test performance distributions is zero \n",
    "\"\"\"\n",
    "size_idx = 0\n",
    "\n",
    "print(\"Train Set Size\", trainsizes[size_idx])\n",
    "\n",
    "# LINEAR\n",
    "# Note: there's only 1 parameter set for the LinearRegression model\n",
    "lnr_res = lnr_crossval.results[0]['results'][size_idx]['test']\n",
    "lnr_test_res = np.mean(lnr_res[metric], axis=1)\n",
    "\n",
    "# RIDGE\n",
    "# Obtain index of best parameters for specified train size\n",
    "r_bp_idx = r_crossval.best_param_inds[size_idx]\n",
    "# Obtain all results for the best parameter set specified train size\n",
    "ridge_res = # TODO\n",
    "# Compute the mean of the outputs for each data set rotation\n",
    "ridge_test_res = # TODO\n",
    "\n",
    "# LASSO\n",
    "l_bp_idx = # TODO\n",
    "lasso_res = # TODO\n",
    "lasso_test_res = # TODO\n",
    "\n",
    "# TODO: ELASTICNET\n",
    "bp_idx = # TODO\n",
    "net_res = # TODO\n",
    "elastic_test_res = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtUYgpl_A_y3",
    "outputId": "d8292097-49d5-42ea-bf3f-6d340c4eb0d5"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "ELASTICNET vs RIDGE\n",
    "Execute the paired t-test to determine whether to reject the null hypothesis \n",
    "(i.e. H0) with 95% confidence. H0 is that the mean of the distribution of the \n",
    "differences between test scores for the best ELASTICNET model and the best \n",
    "RIDGE is zero, when using a training size of 8 (i.e. the size at index 7 of \n",
    "the trainsizes list). Display the t-statistic, the p-value, and the mean of \n",
    "the differences (i.e. mean(elastic_test_res - ridge_test_res))\n",
    "\n",
    "Use stats.ttest_rel(). See the API reference above.\n",
    "Do the same for all the pairing of models\n",
    "\"\"\"\n",
    "stats.ttest_rel(elastic_test_res, ridge_test_res, nan_policy='omit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKiHsDwEA_y3",
    "outputId": "465a408d-654b-4f6c-dd4f-43030de7134f"
   },
   "outputs": [],
   "source": [
    "# PROVIDED\n",
    "mean_of_diffs = np.mean(elastic_test_res - ridge_test_res) \n",
    "mean_of_diffs * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcjFAcwaA_y3",
    "outputId": "d7c9ba3b-8938-43a0-a81e-08af4f7382b1"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET vs LASSO\n",
    "Execute the paired t-test\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xH3kcdi1A_y4",
    "outputId": "2ebf0e50-17f0-45de-b1dd-6de365432788"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the difference between elastic net and lasso\n",
    "mean_of_diffs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPy2NadlA_y4",
    "outputId": "b7e50aeb-7319-4ad3-bec6-8dec0613a758"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET vs LinearRegression\n",
    "Execute the paired t-test\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpDFJGgmA_y4",
    "outputId": "32ebbd1f-e238-4914-8704-00248baab773"
   },
   "outputs": [],
   "source": [
    "# TODO: compute the difference between elastic net and the pure linear model\n",
    "mean_of_diffs = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghW5g8HgA_y7"
   },
   "source": [
    "# DISCUSSION\n",
    "For each question write one brief paragraph of discussion:\n",
    "\n",
    "1. Interpret the meaning of the t-test results using 95% confidence for the one training fold case. Discuss the statistical meaning, as well as the practical interpretation of the results in the context of the data set.\n",
    "\n",
    "## TODO\n",
    "\n",
    "2. Interpret the meaning of the t-test results using 95% confidence for the five training fold case.\n",
    "\n",
    "## TODO\n",
    "\n",
    "3. For the Elastic Net Model, discuss the differences in the surfaces between the train sizes of 1, 3, and 13 folds, for both the training and validation sets.\n",
    "\n",
    "# TODO\n",
    "\n",
    "4. For each of the train set sizes of 1, 2, 13, 18 folds, which model (Linear, Lasso, Ridge, or ElasticNet) and corresponding parameter set would you select and why? Specify which model and parameter set for each size.  Remember, selections should be made based on the validation performance.\n",
    "\n",
    "## TODO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sq9rc3NkA_yi",
    "6UrsX7ctA_yj",
    "acmKQcEcA_yp",
    "XRnAlAoPA_y1"
   ],
   "name": "homework7_sol.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "232px",
    "width": "295px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
