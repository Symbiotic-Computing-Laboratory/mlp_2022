{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPDFDCODllui"
   },
   "source": [
    "__NAME:__ __FULLNAME__  \n",
    "__SECTION:__ __NUMBER__  \n",
    "__CS 5703: Machine Learning Practice__\n",
    "\n",
    "# Homework 10: FORESTS AND BOOSTING\n",
    "\n",
    "## Assignment Overview\n",
    "Follow the TODOs and read through and understand any provided code.  \n",
    "If you have any questions, please post them to Slack.\n",
    "\n",
    "### Task\n",
    "For this assignment you will be exploring Random Forests and Boosting for the purposes of distinguishing Tropical Storms from Tropical Depression given raw data.\n",
    "\n",
    "### [Data set](https://www.kaggle.com/noaa/hurricane-database)\n",
    "The dataset is based on cyclone weather data from NOAA.  \n",
    "You can obtain the data from the server and git under datasets/cyclones.\n",
    "\n",
    "We will be predicting whether a cyclone status is a tropical depression (TD) or not.  \n",
    "Status can be the following types:  \n",
    "* TD – tropical depression  \n",
    "* TS – tropical storm   \n",
    "* HU – hurricane intensity  \n",
    "* EX – Extratropical cyclone  \n",
    "* SD – subtropical depression intensity  \n",
    "* SS – subtropical storm intensity  \n",
    "* LO – low, neither a tropical, subtropical, nor extratropical cyclone  \n",
    "* WV – Tropical Wave  \n",
    "* DB – Disturbance  \n",
    "\n",
    "\n",
    "### Objectives\n",
    "Gain experience with:\n",
    "* DecisionTreeClassifiers\n",
    "* RandomForests\n",
    "* Boosting\n",
    "\n",
    "### General References\n",
    "* [Guide to Jupyter](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Python Built-in Functions](https://docs.python.org/3/library/functions.html)\n",
    "* [Python Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "* [Numpy Reference](https://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "* [Numpy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [Summary of matplotlib](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* [DataCamp: Matplotlib](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264365&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=9026223&gclid=CjwKCAjw_uDsBRAMEiwAaFiHa8xhgCsO9wVcuZPGjAyVGTitb_-fxYtkBLkQ4E_GjSCZFVCqYCGkphoCjucQAvD_BwE)\n",
    "* [Pandas DataFrames](https://urldefense.proofpoint.com/v2/url?u=https-3A__pandas.pydata.org_pandas-2Ddocs_stable_reference_api_pandas.DataFrame.html&d=DwMD-g&c=qKdtBuuu6dQK9MsRUVJ2DPXW6oayO8fu4TfEHS8sGNk&r=9ngmsG8rSmDSS-O0b_V0gP-nN_33Vr52qbY3KXuDY5k&m=mcOOc8D0knaNNmmnTEo_F_WmT4j6_nUSL_yoPmGlLWQ&s=h7hQjqucR7tZyfZXxnoy3iitIr32YlrqiFyPATkW3lw&e=)\n",
    "* [Sci-kit Learn Trees](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)\n",
    "* [Sci-kit Learn Ensemble Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Sci-kit Learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Sci-kit Learn Model Selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "* [Sci-kit Learn Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "* [Sci-kit Learn Preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "* [Decision Trees](https://medium.com/machine-learning-101/chapter-3-decision-trees-theory-e7398adac567)\n",
    "### Hand-In Procedure\n",
    "* Execute all cells so they are showing correct results\n",
    "* Notebook (from Jupyter or Colab):\n",
    "  + Submit this file (.ipynb) to the Gradescope Notebook HW10 dropbox\n",
    "* Note: there is no need to submit a PDF file or to submit directly to Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ykas5JyGlluy"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "import os, re, fnmatch\n",
    "import pathlib, itertools, time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patheffects as peffects\n",
    "import time as timelib\n",
    "\n",
    "from math import ceil, floor\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import f1_score, mean_squared_error, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve\n",
    "import joblib\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (6,5)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['figure.constrained_layout.use'] = False\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZ9AZoy7nU5H",
    "outputId": "ea3f735c-e76d-45f1-9801-62dc939133b5"
   },
   "outputs": [],
   "source": [
    "# COLAB ONLY\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZ9AZoy7nU5H",
    "outputId": "ea3f735c-e76d-45f1-9801-62dc939133b5"
   },
   "outputs": [],
   "source": [
    "# COLAB ONLY\n",
    "## We've discovered a better way to do imports into colab\n",
    "## Now, instead of executing the files, we will copy them\n",
    "## into your colab VM, then import them as normal\n",
    "import pathlib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "#folder = \".\"\n",
    "# TODO: set to your Google Drive folder that contains the py files\n",
    "folder = '/content/drive/MyDrive/hw10'\n",
    "\n",
    "# Copy library python files to local directory\n",
    "for n in pathlib.Path(folder).iterdir():\n",
    "  if re.search(r'.*\\.py', n.name):\n",
    "    shutil.copy(n, n.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hODlAM31oqFN"
   },
   "outputs": [],
   "source": [
    "# LOCAL MACHINES AND COLAB\n",
    "import visualize\n",
    "import metrics_plots\n",
    "from pipeline_components import DataSampleDropper, DataFrameSelector\n",
    "from pipeline_components import DataScaler, DataLabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chl2PgOallve"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Functions for exporting trees to .dot and .pngs\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "def image_combine(ntrees, big_name='big_tree.png', fname_fmt='tree_%02d.png'):\n",
    "    '''\n",
    "    Function for combining some of the trees in the forest into on image\n",
    "    Amalgamate the pngs of the trees into one big image\n",
    "    PARAMS:\n",
    "        ntrees: number of trees from the ensemble to export\n",
    "        big_name: file name for the png containing all ntrees\n",
    "        fname_fmt: file name format string used to read the exported files\n",
    "    '''\n",
    "    # Read the pngs\n",
    "    imgs = [Image.open(fname_fmt % x) for x in range(ntrees)]\n",
    "\n",
    "    # Determine the individual and total sizes\n",
    "    widths, heights = zip(*(i.size for i in imgs))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    # Create the combined image\n",
    "    big_img = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for im in imgs:\n",
    "        big_img.paste(im, (x_offset, 0))\n",
    "        x_offset += im.size[0]\n",
    "    big_img.save(big_name) \n",
    "    print(\"Created %s\" % big_name)\n",
    "    return big_img\n",
    "\n",
    "def export_trees(forest, ntrees=3, fname_fmt='tree_%02d'):\n",
    "    '''\n",
    "    Write trees into inidividual files from the forest\n",
    "    PARAMS:\n",
    "        forest: ensemble of trees classifier\n",
    "        ntrees: number of trees from the ensemble to export\n",
    "        fname_fmt: file name format string used to name the exported files\n",
    "    '''\n",
    "    for t in range(ntrees):\n",
    "        estimator = forest.estimators_[t]\n",
    "        basename = fname_fmt % t\n",
    "        fname = basename + '.dot'\n",
    "        pngname = basename + '.png'\n",
    "        export_graphviz(estimator, out_file=fname, rounded=True, filled=True)\n",
    "        # Command line instruction to execute dot and create the image\n",
    "        !dot -Tpng {fname} > {pngname}\n",
    "        print(\"Created %s and %s\" % (fname, pngname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhQnu9ABllvD",
    "outputId": "bfb47a30-08ab-4786-8345-12ab21a53c89"
   },
   "outputs": [],
   "source": [
    "''' PROVIDED\n",
    "\n",
    "Data set organization functions\n",
    "\n",
    "'''\n",
    "def to_numerical(coord):\n",
    "    '''\n",
    "    Convert Latitude and Longitude into numerical values\n",
    "\n",
    "    '''\n",
    "    \n",
    "    direction = re.findall(r'[NSWE]' , coord)[0]\n",
    "    num = re.match('[\\d]{1,3}.[\\d]{0,1}' , coord)[0]\n",
    "    \n",
    "    # North and East are positive directions\n",
    "    if direction in ['N', 'E']:\n",
    "        return float(num)\n",
    "    return -1. * float(num)\n",
    "\n",
    "\n",
    "def clean_data_set(df, classes=['TD', 'HU', 'TS'], fix_columns=[]):\n",
    "    \"\"\" PROVIDED\n",
    "    Make adjustments to the data.\n",
    "\n",
    "    For wind speed, NaNs are current represented by -999.\n",
    "    We will replace these with NaN.\n",
    "\n",
    "    For Latitude and Longitude, these are strings such as \n",
    "    28.0W. We will replace these with numerical values where\n",
    "    positive directions are N and E, and negative directions \n",
    "    are S and W.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert -999 values to NaNs. These are missing values\n",
    "    NaNvalue = -999\n",
    "    df = df.replace(NaNvalue, np.nan).copy()\n",
    "    \n",
    "    # Interpolate NaNs for columns in fix_columns\n",
    "    for c in fix_columns:\n",
    "        med = df[c].median()\n",
    "        df[c] = df[c].fillna(med)\n",
    "\n",
    "    # Set the datatype of the categorical attributes\n",
    "    cate_attribs = ['Event', 'Status']\n",
    "    df[cate_attribs] = df[cate_attribs].astype('category')\n",
    "\n",
    "    # Set the datatype of the Data attribute to datetime64[ns]\n",
    "    df['Date'] = df['Date'].astype('datetime64[ns]')\n",
    "    \n",
    "    # Clean up lat/long\n",
    "    df['Latitude'] = df['Latitude'].apply(to_numerical)\n",
    "    df['Longitude'] = df['Longitude'].apply(to_numerical)\n",
    "\n",
    "    # class label is defined by the order in the classes parameter\n",
    "    # All other labels will be NaN\n",
    "    \n",
    "    for i,c in enumerate(classes):\n",
    "        isclass = df['Status'].str.contains(c)\n",
    "        df.loc[isclass, 'label'] = i\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhQnu9ABllvD",
    "outputId": "bfb47a30-08ab-4786-8345-12ab21a53c89"
   },
   "outputs": [],
   "source": [
    "''' PROVIDED\n",
    "\n",
    "Performance report generator\n",
    "'''\n",
    "def generate_performance_report(model, Xtrain, ytrain, \n",
    "                                Xval, yval, targetnames):\n",
    "    '''\n",
    "    Produce a performance report for a model as a function of the training\n",
    "    and validation data sets.  Includes:\n",
    "    - Confusion matrices\n",
    "    - ROC and PR-ROC curves\n",
    "    '''\n",
    "    \n",
    "    # Compute the model's predictions.\n",
    "    preds = model.predict(Xtrain)\n",
    "    preds_val = model.predict(Xval)\n",
    "\n",
    "    # Compute the prediction probabilities. \n",
    "    proba = model.predict_proba(Xtrain)\n",
    "    proba_val = model.predict_proba(Xval)\n",
    "\n",
    "    # Compute the model's mean accuracy. \n",
    "    score = model.score(Xtrain, ytrain) \n",
    "    score_val = model.score(Xval, yval)\n",
    "    \n",
    "    print(\"Training Score: %.4f\" % score)\n",
    "    print(\"Validation Score %.4f\" % score_val)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cmtx = confusion_matrix(ytrain, preds)\n",
    "    cmtx_val = confusion_matrix(yval, preds_val)\n",
    "    metrics_plots.confusion_mtx_colormap(cmtx, targetnames, targetnames)\n",
    "    metrics_plots.confusion_mtx_colormap(cmtx_val, targetnames, targetnames)\n",
    "\n",
    "    # KS, ROC, and PRC Curves\n",
    "    roc_prc_results = metrics_plots.ks_roc_prc_plot(ytrain, proba[:,1])\n",
    "    roc_prc_results_val = metrics_plots.ks_roc_prc_plot(yval, proba_val[:,1])\n",
    "\n",
    "    # Compute the PSS and F1 Score\n",
    "    pss_val = metrics_plots.skillScore(yval, preds_val)\n",
    "    f1_val = f1_score(yval, preds_val)\n",
    "    print(\"Val PSS: %.4f\" % pss_val[0])\n",
    "    print(\"Val F1 Score %.4f\" % f1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps7PWDalllvB"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFFY89kxllvC",
    "outputId": "c55d7e2e-5723-494d-afa2-df5deda004db"
   },
   "outputs": [],
   "source": [
    "# TODO: set appropriately\n",
    "\n",
    "filename_val = '/content/drive/MyDrive/MLP_2022/datasets/cyclones/pacific.csv'\n",
    "#filename_val = 'cyclones/pacific.csv'\n",
    "filename_tr = '/content/drive/MyDrive/MLP_2022/datasets/cyclones/atlantic.csv'\n",
    "#filename_tr = 'cyclones/atlantic.csv'\n",
    "\n",
    "\n",
    "# Read both files\n",
    "cyclones_val = pd.read_csv(filename_val)\n",
    "nRows, nCols = cyclones_val.shape\n",
    "print(f'Validation: {nRows} rows and {nCols} columns')\n",
    "\n",
    "cyclones_tr = pd.read_csv(filename_tr)\n",
    "nRows, nCols = cyclones_tr.shape\n",
    "print(f'Training: {nRows} rows and {nCols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhQnu9ABllvD",
    "outputId": "bfb47a30-08ab-4786-8345-12ab21a53c89"
   },
   "outputs": [],
   "source": [
    "cyclones_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "C8_Ed4RIllvG",
    "outputId": "7e8ee037-8d90-4e14-a756-f1837e7dd188"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Clean up the data frames\n",
    "\"\"\"\n",
    "targetnames = ['TS', 'TD']\n",
    "inter_columns = ['Maximum Wind', 'Minimum Pressure', 'Low Wind NE',\n",
    "       'Low Wind SE', 'Low Wind SW', 'Low Wind NW', 'Moderate Wind NE',\n",
    "       'Moderate Wind SE', 'Moderate Wind SW', 'Moderate Wind NW',\n",
    "       'High Wind NE', 'High Wind SE', 'High Wind SW', 'High Wind NW']\n",
    "\n",
    "df_val = clean_data_set(cyclones_val, classes=targetnames, fix_columns=inter_columns)\n",
    "df_tr = clean_data_set(cyclones_tr, classes=targetnames, fix_columns=inter_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_887hshzllvJ",
    "outputId": "6fa9611d-4c57-4ee9-f68f-5acb08df46d2"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display the quantitiy of NaNs for each feature\n",
    "\"\"\"\n",
    "df_tr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "wosEIYUfllvM",
    "outputId": "4b749f65-02b5-46c6-97bc-c4c8baf82609"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display summary statistics for each feature of the dataframe\n",
    "\"\"\"\n",
    "df_tr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcF6Q8QvllvO"
   },
   "source": [
    "# PRE-PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47auI8DTllvP",
    "outputId": "d30a7e5d-885e-4c26-a44b-892c733986d5"
   },
   "outputs": [],
   "source": [
    "df_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVC3oqwFllvR"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Construct preprocessing pipeline\n",
    "\"\"\"\n",
    "# Features to use for prediction + the predictor (last item)\n",
    "selected_features = ['Latitude', 'Longitude', \n",
    "                     'Low Wind NE',\n",
    "                     'Low Wind SE', \n",
    "                     'Low Wind SW',\n",
    "                     'Low Wind NW',\n",
    "                     'Moderate Wind NE', \n",
    "                     'Minimum Pressure',\n",
    "                     'Moderate Wind SE', \n",
    "                     'Moderate Wind NE', \n",
    "                     'Moderate Wind NW',\n",
    "                     'Moderate Wind SW',\n",
    "                     'High Wind NE', \n",
    "                     'High Wind NW',\n",
    "                     'High Wind SE',\n",
    "                     'label']\n",
    "\n",
    "# Pipeline for filtering the data\n",
    "pipe = Pipeline([\n",
    "    ('FeatureSelector', DataFrameSelector(selected_features)),\n",
    "    ('RowDropper', DataSampleDropper())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hC4pwYscllvS",
    "outputId": "ebd1a55d-a257-416c-a4f2-2f240d8ff115"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Pre-process the data using the defined pipeline\n",
    "\"\"\"\n",
    "tr_data = pipe.fit_transform(df_tr)\n",
    "nsamples, ncols = tr_data.shape\n",
    "nsamples, ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBUcogXmllvT",
    "outputId": "38663639-980f-40b6-f64b-98d913f2582b"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Pre-process the data using the defined pipeline\n",
    "\"\"\"\n",
    "val_data = pipe.fit_transform(df_val)\n",
    "nsamples, ncols = val_data.shape\n",
    "nsamples, ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBUcogXmllvT",
    "outputId": "38663639-980f-40b6-f64b-98d913f2582b"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Verify all NaNs removed\n",
    "\"\"\"\n",
    "tr_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBUcogXmllvT",
    "outputId": "38663639-980f-40b6-f64b-98d913f2582b"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Verify all NaNs removed\n",
    "\"\"\"\n",
    "val_data.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuBdSNHellvV"
   },
   "source": [
    "# VISUALIZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "id": "jyMK1QInllva",
    "outputId": "03840c83-0f1c-4298-ab27-e8f26028841e"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display the Pearson correlation between all pairs of the features\n",
    "use visualize.scatter_corrplots\n",
    "\"\"\"\n",
    "cdata = tr_data.astype('float64').copy()\n",
    "visualize.scatter_corrplots(cdata.values, cdata.columns, corrfmt=\"%.1f\", FIGW=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy5itWMrllvd"
   },
   "source": [
    "## Reflection #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy5itWMrllvd"
   },
   "source": [
    "a.  Which features do you expect to be most relevent for predicting the label?\n",
    "\n",
    "__TODO__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy5itWMrllvd"
   },
   "source": [
    "# Create Training and Validation Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8GfPs_Cllvf",
    "outputId": "7ed0564f-baee-4b5e-ebd3-f7f7f83d3959"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Create the training and validation data sets\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X = tr_data.drop(['label'], axis=1).values\n",
    "y = tr_data['label'].astype('int64').values\n",
    "\n",
    "# We originally were planning to use the other data set as validation, but \n",
    "#  the atlantic and pacific are very different conditions\n",
    "\n",
    "#Xval = val_data.drop(['label'], axis=1).copy()\n",
    "#yval = val_data['label'].astype('int64').copy()\n",
    "\n",
    "# Subsample the training set\n",
    "#Xtrain, Xval, ytrain, yval = train_test_split(X, y, stratify=y, test_size=0.5)#, random_state=42)\n",
    "\n",
    "# Because there is temporal autocorrelation, we are just splitting \n",
    "#  the data into a training set and a validation set\n",
    "split = 14000\n",
    "Xtrain = X[:split,:]\n",
    "ytrain = y[:split]\n",
    "Xval = X[split:,:]\n",
    "yval = y[split:]\n",
    "Xtrain.shape, ytrain.shape, Xval.shape, yval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4K0c1erllvg"
   },
   "source": [
    "# DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pz0oAXl9llvh",
    "outputId": "b76b05ff-3930-4082-f2e8-aca47972777f"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Create and train DecisionTree for comparision with the ensemble methods \n",
    "\n",
    "Select appropriate parameters for the Decision Tree Classifier\n",
    "\n",
    "\"\"\"\n",
    "tree_clf = DecisionTreeClassifier( TODO )\n",
    "tree_clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tDhVc1u6llvi",
    "outputId": "3eac3312-d97a-4b40-e7f8-3d772e6fb46d"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Compute the predictions, prediction probabilities, and the accuracy scores\n",
    "for the training and validation sets for the learned instance of the model\n",
    "\n",
    "Display the confusion matrix, KS plot, ROC curve, and PR curve for the training \n",
    "and validation sets using metrics_plots.ks_roc_prc_plot\n",
    "\n",
    "The red dashed line in the ROC and PR plots are indicative of the expected \n",
    "performance for a random classifier, which would predict postives at the \n",
    "rate of occurance within the data set\n",
    "\"\"\"\n",
    "\n",
    "generate_performance_report(tree_clf, Xtrain, ytrain,\n",
    "                            Xval, yval,\n",
    "                            targetnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pM0gOzF2llvj"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Export the tree as a .dot file and create the png\n",
    "\"\"\"\n",
    "fname = 'tree.dot'\n",
    "pngname = 'tree.png'\n",
    "export_graphviz(tree_clf, feature_names=selected_features[:-1],\n",
    "                class_names=targetnames, out_file=fname, \n",
    "                rounded=True, filled=True)\n",
    "\n",
    "# If the following command does not work, you can manually convert\n",
    "# the dot file into a png here: \n",
    "#  https://onlineconvertfree.com/convert-format/dot-to-png/\n",
    "!dot -Tpng {fname} -o {pngname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "f9jWx44Hllvl",
    "outputId": "a4830ad8-948c-4e5c-9ef5-90b8cb6f464d"
   },
   "outputs": [],
   "source": [
    "display.Image(\"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXDSrNqyllvm"
   },
   "source": [
    "## Reflection #2\n",
    "\n",
    "a. Compare the performance between small, medium-sized and large trees with respect to the validation set.\n",
    "\n",
    "__TODO__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXDSrNqyllvm"
   },
   "source": [
    "# RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJx0z2k3llvp",
    "outputId": "57c21ef8-4bf8-4c46-baa3-f1e9125030da"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Create and train a RandomForest\n",
    "Explore various configurations of the hyper-parameters. \n",
    "\n",
    "Train the models on the training set and evaluate them for the training and\n",
    "validation sets.\n",
    "\n",
    "Examine the API and the book for the meaning and impact of different \n",
    "hyper-parameters\n",
    "\"\"\"\n",
    "forest_clf = RandomForestClassifier( TODO )\n",
    "forest_clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGd5sjDEllvs",
    "outputId": "e684f6b3-8ba6-40c2-bc4c-73d62e662fdf"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Export some trees from your favorite model as a .dot file\n",
    "We can use the estimators_ attribute of the forest to get a list of the trees\n",
    "\n",
    "Amalgamate the pngs of the trees into one big image\n",
    "\"\"\"\n",
    "ntrees = 2\n",
    "\n",
    "'''\n",
    "This will work on colab\n",
    "\n",
    "If running on a local machine, and if the dot command does not work on your computer,\n",
    "please modify the export_trees function by commenting out the line where the dot \n",
    "command is being invokedThen you can manually convert each dot file into a png file \n",
    "at the following website:\n",
    "https://onlineconvertfree.com/convert-format/dot-to-png/\n",
    "After converting all of the dot files into a png, you should be able to use the \n",
    "image_comibne() function\n",
    "'''\n",
    "export_trees(forest_clf, #feature_names=X.columns, class_names=targetnames, rounded=True, filled=True, \n",
    "             fname_fmt='e_rf_model_%02d')\n",
    "big_img = image_combine(ntrees, big_name='e_rf_model.png', \n",
    "                        fname_fmt='e_rf_model_%02d.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "TdtgP4Ocllvu",
    "outputId": "7c64565d-5237-4c9d-a922-8a88ec233a7a"
   },
   "outputs": [],
   "source": [
    "''' PROVIDED\n",
    "Display the tree file\n",
    "'''\n",
    "display.Image(\"e_rf_model.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVgps8Blllvv",
    "tags": []
   },
   "source": [
    "### TRAINING AND VALIDATION RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X1hQ8flBllvx",
    "outputId": "9acef9ee-0dd5-4c49-cd66-9655515e8c41"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Compute the predictions, prediction probabilities, and the accuracy scores\n",
    "for the training and validation sets for the learned instance of the model\n",
    "\n",
    "Display the confusion matrix, KS plot, ROC curve, and PR curve for the training \n",
    "and validation sets using metrics_plots.ks_roc_prc_plot\n",
    "\n",
    "The red dashed line in the ROC and PR plots are indicative of the expected \n",
    "performance for a random classifier, which would predict postives at the \n",
    "rate of occurance within the data set\n",
    "\"\"\"\n",
    "\n",
    "generate_performance_report(forest_clf, Xtrain, ytrain,\n",
    "                            Xval, yval,\n",
    "                            targetnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cqCT4Nxllvz"
   },
   "source": [
    "# ADABOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7nq3Swqllv1",
    "outputId": "7cffc20a-835c-4fe4-ad01-10584a11d5e3"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Create and train a Boosting model \n",
    "\n",
    "Explore various boosting models to improve your validation performance.\n",
    "Train the models on the training set and evaluate them for the training and\n",
    "validation sets. Try boosting the benmark tree_clf \n",
    "\"\"\"\n",
    "tree_clf2 = DecisionTreeClassifier( TODO )\n",
    "\n",
    "ada_clf = AdaBoostClassifier( TODO )\n",
    "\n",
    "ada_clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7nq3Swqllv1",
    "outputId": "7cffc20a-835c-4fe4-ad01-10584a11d5e3"
   },
   "source": [
    "### TRAINING AND VALIDATION RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7nq3Swqllv1",
    "outputId": "7cffc20a-835c-4fe4-ad01-10584a11d5e3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the predictions, prediction probabilities, and the accuracy scores\n",
    "for the training and validation sets for the learned instance of the model\n",
    "\n",
    "Display the confusion matrix, KS plot, ROC curve, and PR curve for the training \n",
    "and validation sets using metrics_plots.ks_roc_prc_plot\n",
    "\n",
    "The red dashed line in the ROC and PR plots are indicative of the expected \n",
    "performance for a random classifier, which would predict postives at the \n",
    "rate of occurance within the data set\n",
    "\"\"\"\n",
    "\n",
    "generate_performance_report(ada_clf, Xtrain, ytrain,\n",
    "                            Xval, yval,\n",
    "                            targetnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7nq3Swqllv1",
    "outputId": "7cffc20a-835c-4fe4-ad01-10584a11d5e3"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Export some trees from your favorite model as a .dot file\n",
    "We can use the estimators_ attribute of the forest to get a list of the trees\n",
    "\n",
    "Amalgamate the pngs of the trees into one big image\n",
    "\"\"\"\n",
    "ntrees = 2\n",
    "\n",
    "'''\n",
    "This will work on colab\n",
    "\n",
    "If running on a local machine, and if the dot command does not work on your computer,\n",
    "please modify the export_trees function by commenting out the line where the dot \n",
    "command is being invokedThen you can manually convert each dot file into a png file \n",
    "at the following website:\n",
    "https://onlineconvertfree.com/convert-format/dot-to-png/\n",
    "After converting all of the dot files into a png, you should be able to use the \n",
    "image_comibne() function\n",
    "'''\n",
    "export_trees(ada_clf, #feature_names=X.columns, class_names=targetnames, rounded=True, filled=True, \n",
    "             fname_fmt='e_ada_model_%02d')\n",
    "big_img = image_combine(ntrees, big_name='e_ada_model.png', \n",
    "                        fname_fmt='e_ada_model_%02d.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7nq3Swqllv1",
    "outputId": "7cffc20a-835c-4fe4-ad01-10584a11d5e3"
   },
   "outputs": [],
   "source": [
    "''' PROVIDED\n",
    "Display the tree file\n",
    "'''\n",
    "display.Image(\"e_ada_model.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ3MNwBIllv7"
   },
   "source": [
    "## Reflection #3\n",
    "\n",
    "a. Compare the PR_AUC validation performance for your best single decision tree to both a reasonable forest and a reasonable ada-boosted forest.\n",
    "\n",
    "__TODO__\n",
    "\n",
    "\n",
    "\n",
    "b.  Explain why Boosting shows an improvement in performance relative to the Random Forest (or, at least should, in many problems).\n",
    "\n",
    "__TODO__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ3MNwBIllv7"
   },
   "source": [
    "# FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "x2XaIJe8llv8",
    "outputId": "f272ccb4-a01f-4f5d-b16e-d26223cf6c7f"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display the feature imporantances\n",
    "see the API for RandomForests and boosted tree\n",
    "you can create a DataFrame to help with the display\n",
    "\"\"\"\n",
    "feature_imp = pd.DataFrame([tree_clf.feature_importances_, \n",
    "                            forest_clf.feature_importances_,\n",
    "                            ada_clf.feature_importances_], \n",
    "                           columns=selected_features[:-1], \n",
    "                           index=['DecisionTree', 'RandomForest', 'AdaBoosting']).T\n",
    "feature_imp.plot.bar()\n",
    "plt.xlabel('Feature Name')\n",
    "plt.ylabel('Fraction of Importance')\n",
    "plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxqMYMpDllv-"
   },
   "source": [
    "## Reflection #4\n",
    "\n",
    "a. Which features were most important for all three models?\n",
    "\n",
    "__TODO__\n",
    "\n",
    "\n",
    "\n",
    "b. Which features show the biggest differences in importance across the three models?\n",
    "\n",
    "__TODO__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYACJ2fDllv_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mVgps8Blllvv",
    "HVAZ56Yullv2"
   ],
   "name": "homework10_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
