{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NAME:__ __FULLNAME__  \n",
    "__SECTION:__ __NUMBER__  \n",
    "__CS 5703: Machine Learning Practices__\n",
    "\n",
    "# Homework 11: Dimensionality Reduction\n",
    "\n",
    "## Assignment Overview\n",
    "Follow the TODOs and read through and understand any provided code.  \n",
    "For all plots, make sure all necessary axes and curves are clearly and \n",
    "accurately labeled. Include figure/plot titles appropriately as well.\n",
    "\n",
    "\n",
    "### Task\n",
    "For this assignment you will be exploring dimensionality reduction using\n",
    "Prinicipal Componenet Analysis (PCA). Having a large number of features \n",
    "can dramatically increase training times and the likelihood of overfitting.\n",
    "Additionally, it's difficult to visualize and understand patterns in high \n",
    "dimensional spaces. It's not uncommon that a lower dimensional subspace\n",
    "of the full feature space will better characterize trends within the data.\n",
    "PCA is one such technique that attempts to locate such subspaces and projects\n",
    "the data into the determined subspace.\n",
    "\n",
    "\n",
    "### Data set   \n",
    "Heart Arrhythmia: distinguishing Normal vs Abnormal arrhythmia.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "Gain experience in using:\n",
    "* Dimensionality Reduction\n",
    "* Principal Component Analysis (PCA)\n",
    "* PCA as a preprocessing step to a classifier\n",
    "\n",
    "\n",
    "### General References\n",
    "* [Guide to Jupyter](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Python Built-in Functions](https://docs.python.org/3/library/functions.html)\n",
    "* [Python Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "* [Numpy Reference](https://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "* [Numpy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [Summary of matplotlib](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* [DataCamp: Matplotlib](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264365&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=9026223&gclid=CjwKCAjw_uDsBRAMEiwAaFiHa8xhgCsO9wVcuZPGjAyVGTitb_-fxYtkBLkQ4E_GjSCZFVCqYCGkphoCjucQAvD_BwE)\n",
    "* [Pandas DataFrames](https://urldefense.proofpoint.com/v2/url?u=https-3A__pandas.pydata.org_pandas-2Ddocs_stable_reference_api_pandas.DataFrame.html&d=DwMD-g&c=qKdtBuuu6dQK9MsRUVJ2DPXW6oayO8fu4TfEHS8sGNk&r=9ngmsG8rSmDSS-O0b_V0gP-nN_33Vr52qbY3KXuDY5k&m=mcOOc8D0knaNNmmnTEo_F_WmT4j6_nUSL_yoPmGlLWQ&s=h7hQjqucR7tZyfZXxnoy3iitIr32YlrqiFyPATkW3lw&e=)\n",
    "* [Sci-kit Learn Linear Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Sci-kit Learn Ensemble Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Sci-kit Learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Sci-kit Learn Model Selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "* [Sci-kit Learn Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "* [Sci-kit Learn Preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "* [SciPy Paired t-test for Dependent Samples](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html)\n",
    "\n",
    "### Hand-In Procedure\n",
    "* Execute all cells so they are showing correct results\n",
    "* Notebook (from Jupyter or Colab):\n",
    "  + Submit this file (.ipynb) to the Gradescope Notebook HW11 dropbox\n",
    "* Note: there is no need to submit a PDF file or to submit directly to Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import metrics_plots\n",
    "from pipeline_components import DataSampleDropper, DataFrameSelector\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time as timelib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, Binarizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (6,5)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['figure.constrained_layout.use'] = False\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute in Colab\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute in Colab\n",
    "\n",
    "## We've discovered a better way to do imports into colab\n",
    "## Now, instead of executing the files, we will copy them\n",
    "## into your colab VM, then import them as normal\n",
    "import pathlib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# TODO: fill in the right folder location\n",
    "folder = '/content/drive/My Drive/TODO'\n",
    "\n",
    "for n in pathlib.Path(folder).iterdir():\n",
    "  if re.search(r'.*\\.py', n.name):\n",
    "    shutil.copy(n, n.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB and Local execution\n",
    "import metrics_plots\n",
    "from pipeline_components import DataSampleDropper, DataFrameSelector, DataSampleSwapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/content/drive/My Drive/MLP_2022/datasets/heart_arrhythmia.csv'\n",
    "#filename = 'heart_arrhythmia.csv'\n",
    "\n",
    "heart = pd.read_csv(filename, delimiter=',', nrows=None)\n",
    "heart.dataframeName = filename\n",
    "\n",
    "nRows, nCols = heart.shape\n",
    "print(f'There are {nRows} rows and {nCols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=heart['diagnosis'].values\n",
    "plt.hist(d)\n",
    "np.sum(d==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Evaluate the training performance of an already trained model. \n",
    "\n",
    "Used to evaluate a PCA model\n",
    "\"\"\"\n",
    "def compute_rmse(x, y):\n",
    "    return np.sqrt(np.nanmean((x - y)**2))\n",
    "\n",
    "\"\"\" PROVIDED\n",
    "Evaluate the training performance of an already trained classifier model\n",
    "\"\"\"\n",
    "def predict_and_score(model, X, y):\n",
    "    '''\n",
    "    Compute the model predictions and cooresponding scores.\n",
    "    PARAMS:\n",
    "        X: feature data\n",
    "        y: corresponding output\n",
    "    RETURNS:\n",
    "        preds: predictions of the model from X\n",
    "        score: score computed by the models score() method\n",
    "        f1: F1 score\n",
    "        \n",
    "    '''\n",
    "    preds = model.predict(X)\n",
    "\n",
    "    f1 = f1_score(y, preds)\n",
    "    score = model.score(X, y)\n",
    "    \n",
    "    return preds, score, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Create a Pipeline to prepare the data\n",
    "\"\"\"\n",
    "# Features to keep in the analysis\n",
    "feature_names_initial = heart.columns.drop(['J'])\n",
    "\n",
    "# Features to keep as inputs to the model\n",
    "feature_names = heart.columns.drop(['diagnosis', 'J'])\n",
    "\n",
    "# Preprocessing pipeline will be a component of the input/output pipelines\n",
    "pipe_pre = Pipeline([\n",
    "    (\"removeAttribs\", DataFrameSelector(feature_names_initial)),\n",
    "    (\"Cleanup\", DataSampleSwapper((('?', np.nan),))),\n",
    "    (\"NaNrowDropper\", DataSampleDropper()),\n",
    "])\n",
    "\n",
    "# Input pipeline\n",
    "pipe_X = Pipeline([\n",
    "    (\"pipe_pre\", pipe_pre),\n",
    "    (\"selectAttribs\", DataFrameSelector(feature_names)),\n",
    "    (\"scaler\", RobustScaler())\n",
    "])\n",
    "\n",
    "# Output pipeline\n",
    "pipe_y = Pipeline([\n",
    "    (\"pipe_pre\", pipe_pre),\n",
    "    (\"selectAttribs\", DataFrameSelector(['diagnosis'])),\n",
    "    #(\"binarizer\", Binarizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Format the data to provide to the models\n",
    "\"\"\"\n",
    "X = pipe_X.fit_transform(heart)\n",
    "y = pipe_y.fit_transform(heart).values.ravel()\n",
    "\n",
    "# y is an int - convert to 1/0\n",
    "# 1.0 = Normal; 0.0 = abnormal\n",
    "y = (y == 1) + 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, stratify=y, \n",
    "                                                test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Abnormal', 'Normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK\n",
    "The task is to predict the normal arrhythmia in patients.\n",
    "We are going to compare the performance of a LogisticRegression model trained on the original data to a LogisticRegression model trained using PCA-transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegresson Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LogisticRegression benchmark for comparision.  \n",
    "\n",
    "Do not use regularization.  Use as many iterations that you need \n",
    "to allow the model to converge to a solution.\n",
    "\"\"\"\n",
    "benchmark_lnr = LogisticRegression(TODO)\n",
    "benchmark_lnr.fit(Xtrain, ytrain)\n",
    "\n",
    "# Compute predictions on fully trained model for train set\n",
    "preds, score, f1 = predict_and_score(benchmark_lnr, Xtrain, ytrain)\n",
    "print(\"Train:\\tF1: %.3f\\tScore: %.3f\" % (f1, score))\n",
    "# Compute predictions on fully trained model for val set\n",
    "preds, score, f1 = predict_and_score(benchmark_lnr, Xtest, ytest)\n",
    "print(\"Test:\\tF1: %.3f\\tScore: %.3f\" % (f1, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection #1\n",
    "\n",
    "What are the F1 scores for both the training and test sets?\n",
    "\n",
    "__TODO__\n",
    "\n",
    "Train: 1.0\n",
    "Test: 0.747 (could vary a little, depending on configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Train a PCA model using the training set with whiten=True\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Examine how much variance is accounted for by each PC.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "How many PCs are necessary to achieve a specified variance?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection #2\n",
    "\n",
    "\n",
    "\n",
    "How many PCs are necessary to account for 90% of the data variance?\n",
    "\n",
    "__TODO__\n",
    "\n",
    "\n",
    "How many PCs are necessary to account for 95%?\n",
    "\n",
    "__TODO__\n",
    "\n",
    "\n",
    "How many PCs are necessary to account for 99% of the data variance?\n",
    "\n",
    "__TODO__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Using the number of PCs obtained for 99% variance, re-fit the PCA with\n",
    "whiten=True and project the training data into PC space\n",
    "\"\"\"\n",
    "\n",
    "#TODO\n",
    "\n",
    "# PROVIDED: Compute the reconstruction error (rmse)\n",
    "compute_rmse(Xtrain, Xtrain_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Implement a model Pipeline. The first step of the pipeline is \n",
    "PCA with n_components set to the number of PCs determined above\n",
    "that account for 99% of the data variance and whiten to true.\n",
    "\n",
    "The second step of the pipeline is LogisticRegression() with no regularization.\n",
    "\"\"\"\n",
    "# TODO: Create Pipeline model\n",
    "\n",
    "# TODO: Fit model to entire train set\n",
    "pca_model.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection #3\n",
    "\n",
    "What are the F1 scores for both the training and test sets for the 80% variance case?\n",
    "\n",
    "__TODO__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRIDSEARCH \n",
    "Use the GridSearchCV class to search for hyper-parameters for the Pipeline object that\n",
    "you created above.  \n",
    "\n",
    "The hyper-parameter you should vary for PCA is n_components.  When creating the hyper-parameter dictionary for a Pipeline object, the hyper-parameter names are of the form: A__B, where A is the Pipeline element name and B is the hyper-parameter name for that pipeline element.  Example: 'Classifier__max_iter'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDED: List of number of PCs to try\n",
    "components = np.linspace(1,100,num=40, dtype=int)\n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Create the GridSearchCV object using the PCA \n",
    "pipeline model created above, and use GridSearchCV with cv=5 and opt_metric='f1'\n",
    "\"\"\"\n",
    "# Grid Search Parameters\n",
    "opt_metric = 'f1'\n",
    "maximize_opt_metric = False\n",
    "CV = 5\n",
    "\n",
    "# GridSearch pipeline hyper-parameters can be specified \n",
    "# with ‘__’ separated parameter names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Display the GridSearch results as a pandas dataframe \n",
    "\"\"\"\n",
    "pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the mean f1 score vs the number of PCs on the train and validation sets \n",
    "for each model, using the 'mean_train_score' and 'mean_test_score'\n",
    "keys of the search.cv_results_ dictionary\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Train the best estimator with the full training set\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection #4\n",
    "\n",
    "What is the optimal number of components with respect to the validation set?\n",
    "\n",
    "__TODO__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What are the F1 scores for both the training and test sets for the 80% variance case?\n",
    "\n",
    "__TODO__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Regularization\n",
    "\n",
    "TODO: \n",
    "1. Use GridSearchCV to find the most appropriate regularization parameter value for LogisticRegression (do not use PCA)\n",
    "\n",
    "2. Refit the training set to the best parameter choice\n",
    "\n",
    "3. Evaluate with respect to the training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection #5\n",
    "\n",
    "Compare the test set performance for the best PCA-LR and the LR-with-regularization models.\n",
    "\n",
    "__TODO__\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
