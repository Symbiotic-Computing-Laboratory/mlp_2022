{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWsdiIFTd6rc"
   },
   "source": [
    " __NAME:__ __FULLNAME__  \n",
    "__SECTION:__ __NUMBER__  \n",
    "__CS 5703: Machine Learning Practices__\n",
    "\n",
    "# Homework 6: Cross Validation\n",
    "\n",
    "## Assignment Overview\n",
    "First read through the entire notebook, do not write any code. This assignment\n",
    "is more complex than previous ones, and it will be helpful to have a sense of \n",
    "the structure before you start coding.  \n",
    "\n",
    "Follow the TODOs and read through and understand any provided code.  \n",
    "All the plotting functions have been provided. You should not need to alter\n",
    "any of these.\n",
    "\n",
    "### Task\n",
    "For this assignment you will be implementing __holistic cross validation__. \n",
    "Cross validation is a procedure that involves training, validating, and testing \n",
    "a model on different subsets of the data set to evaluate how well the model will \n",
    "generalize to unseen examples. Additionally, cross validation is a good tool \n",
    "for evaulating models when only small amounts of data are available.  \n",
    "\n",
    "The train sets are used to train the parameters of the various models; the validation \n",
    "sets are used to evaluate and select the best performing model hyper-parameter set. ### Data set\n",
    "The BMI (Brain Machine Interface) data are stored in a single pickle file; within this file, there\n",
    "is one dictionary that contains all of the data.  The keys are: 'MI', \n",
    "'theta', 'dtheta', 'torque', and 'time'.  Each of these objects are python lists with 20 \n",
    "numpy matrices; each matrix contains an independent fold of data, with rows representing \n",
    "different samples and columns representing different dimensions.  The samples are organized \n",
    "contiguously (one sample every 50ms), but there are gaps in the data.\n",
    "* _MI_ contains the data for 48 neurons.  Each row encodes the number of action potentials for \n",
    "each neuron at each of 20 different time bins (so, 48 x 20 = 960 columns).  \n",
    "* _theta_ contains the angular position of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each sample.  \n",
    "* _dtheta_ contains the angular velocity of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each sample.  \n",
    "* _torque_ contains the torque of the shoulder (in column 0) and the elbow (in column 1) for each sample.  \n",
    "* _time_ contains the actual time stamp of each sample.\n",
    "\n",
    "A fold is a subset of the available data.  Cutting the data into folds is useful for adjusting training, validation, and test \n",
    "sets sizes, and for assessing the generality of a modelling approach.Each fold contains independent time points.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "* Implement and understand __Holistic Cross Validation__\n",
    "* Perform a training set size sensitivity analysis and observe how hyper-parameter choices change with training set size\n",
    "\n",
    "### General References\n",
    "* [Guide to Jupyter](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Python Built-in Functions](https://docs.python.org/3/library/functions.html)\n",
    "* [Python Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "* [Numpy Reference](https://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "* [Numpy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [Summary of matplotlib](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* [DataCamp: Matplotlib](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264365&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=9026223&gclid=CjwKCAjw_uDsBRAMEiwAaFiHa8xhgCsO9wVcuZPGjAyVGTitb_-fxYtkBLkQ4E_GjSCZFVCqYCGkphoCjucQAvD_BwE)\n",
    "* [Pandas DataFrames](https://urldefense.proofpoint.com/v2/url?u=https-3A__pandas.pydata.org_pandas-2Ddocs_stable_reference_api_pandas.DataFrame.html&d=DwMD-g&c=qKdtBuuu6dQK9MsRUVJ2DPXW6oayO8fu4TfEHS8sGNk&r=9ngmsG8rSmDSS-O0b_V0gP-nN_33Vr52qbY3KXuDY5k&m=mcOOc8D0knaNNmmnTEo_F_WmT4j6_nUSL_yoPmGlLWQ&s=h7hQjqucR7tZyfZXxnoy3iitIr32YlrqiFyPATkW3lw&e=)\n",
    "* [Sci-kit Learn Linear Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Sci-kit Learn Ensemble Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Sci-kit Learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Sci-kit Learn Model Selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "\n",
    "## Hand-In Procedure\n",
    "* Execute all cells so they are showing correct results\n",
    "* Notebook (from Jupyter or Colab):\n",
    "  + Submit this file (.ipynb) to the Gradescope Notebook HW6 dropbox\n",
    "* Note: there is no need to submit a PDF file or to submit directly to Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtEpd7j6d6rg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.stats as stats\n",
    "import os, re, fnmatch\n",
    "import pathlib, itertools, time\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "# Default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipXip0EYd6ri",
    "outputId": "d8e67812-862e-4926-d86a-6f634f8c17f9"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-McDJZk8d6rj"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27_0NQCvd6rk",
    "outputId": "448cb63a-58dd-446d-82a2-7a74599751ba"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Load the BMI data from all the folds\n",
    "\"\"\"\n",
    "#fname = '/home/fagg/datasets/bmi/bmi_dataset.pkl'\n",
    "fname = '/content/drive/MyDrive/MLP_2022/datasets/bmi_dataset.pkl'\n",
    "\n",
    "with open(fname, 'rb') as f:\n",
    "  bmi = pkl.load(f)\n",
    "\n",
    "MI_folds = bmi['MI'] \n",
    "theta_folds = bmi['theta']\n",
    "dtheta_folds = bmi['dtheta']\n",
    "torque_folds = bmi['torque']\n",
    "time_folds = bmi['time']\n",
    "\n",
    "alldata_folds = zip(MI_folds, theta_folds, dtheta_folds, torque_folds, time_folds)\n",
    "\n",
    "nfolds = len(MI_folds)\n",
    "nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRH3ZOcJd6rm",
    "outputId": "9670007d-0fe7-4e80-a7e2-38d46882d221"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Print out the shape of all the data for each fold\n",
    "\"\"\"\n",
    "for i, (MI, theta, dtheta, torque, time) in enumerate(alldata_folds):\n",
    "    print(\"FOLD %2d \" % i, MI.shape, theta.shape, \n",
    "          dtheta.shape, torque.shape, time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B642OrHqd6ro",
    "outputId": "8b209a0e-c83c-4f0b-acda-7a4794f51f08"
   },
   "outputs": [],
   "source": [
    "\"\"\" Provided\n",
    "For this homework, it actually becomes rather difficult to work with \n",
    "individual arrays of different length, because numpy cannot efficiently\n",
    "concatenate them together  \n",
    "\n",
    "Here, we construct a single unified array for our data, which we can \n",
    "then be indexed into using the folds_idx array. This allows us to efficiently \n",
    "index into our data instead of creating\n",
    "a copy of it each time we want to change the size of the the training set or\n",
    "our cross-validation rotation.\n",
    "\"\"\"\n",
    "folds_idx = [None]*nfolds\n",
    "unified_idx = 0\n",
    "for i, fold in enumerate(time_folds):\n",
    "    # creates a list containing indexes from start of fold to end of fold,\n",
    "    # eg folds_idx[0] = [0,1,...,1192], folds_idx[1] = [1193,...,2296], ...\n",
    "    # we don't need to store all this (could just store start indexes),\n",
    "    # but this makes it a lot easier later\n",
    "    folds_idx[i] = list(range(unified_idx, unified_idx + fold.shape[0]))\n",
    "    unified_idx += fold.shape[0]\n",
    "\n",
    "def concat_folds(folds):\n",
    "    all_folds = np.array(folds[0])\n",
    "    for f in folds[1:]:\n",
    "        all_folds = np.append(all_folds, f, axis=0)\n",
    "    return all_folds\n",
    "\n",
    "MI = concat_folds(MI_folds) \n",
    "theta = concat_folds(theta_folds)\n",
    "dtheta = concat_folds(dtheta_folds)\n",
    "torque = concat_folds(torque_folds)\n",
    "time = concat_folds(time_folds)\n",
    "\n",
    "print(MI.shape, theta.shape, dtheta.shape, torque.shape, time.shape)\n",
    "[folds_idx[i][0] for i in range(nfolds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEQ9e1dLd6rp"
   },
   "source": [
    "# PARAMETER SET LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iA0CcLFSd6rr"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Construct the Cartesian product of the parameters.\n",
    "\n",
    "This implementation is similar to what is used by GridSearchCV to create its\n",
    "Cartesian product of hyper-parameters.\n",
    "\n",
    "Note that one can also include other hyper-parameters in the list with only a single\n",
    "option.  This has the effect of simply setting the same hyper-parameter value for each\n",
    "of resulting hyper-parameter set.\n",
    "\n",
    "\"\"\"\n",
    "def generate_paramsets(param_lists):\n",
    "    '''\n",
    "    Construct the Cartesian product of the parameters\n",
    "    PARAMS:\n",
    "        params_lists: dict of lists of values to try for each parameter.\n",
    "                      keys of the dict are the names of the parameters\n",
    "                      values are lists of values to try for the \n",
    "                      corresponding parameter\n",
    "    RETURNS: a list of dicts of hyper-parameter sets.  These make up the \n",
    "    Cartesian product of the possible hyper-parameters\n",
    "    '''\n",
    "    keys, values = zip(*param_lists.items())\n",
    "    \n",
    "    # Determines cartesian product of parameter values\n",
    "    combos = itertools.product(*values)\n",
    "    \n",
    "    # Constructs list of dictionaries\n",
    "    combos_dicts = [dict(zip(keys, vals)) for vals in combos]\n",
    "    return list(combos_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NskWaATd6rs"
   },
   "source": [
    "# PERFORMANCE EVALUTION\n",
    "Tools for evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmGUQ1R_d6rt"
   },
   "outputs": [],
   "source": [
    "def mse_rmse(trues, preds):\n",
    "    ''' PROVIDED\n",
    "    Compute MSE and rMSE for each column separately.\n",
    "    '''\n",
    "    mse = np.sum(np.square(trues - preds), axis=0) / trues.shape[0]\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse, rmse\n",
    "\n",
    "def score_eval(X, y, preds):\n",
    "    ''' PROVIDED\n",
    "    Compute the  performance scores for the reulsts produced by a model\n",
    "    PARAMS:\n",
    "        X: input feature data\n",
    "        y: true output for X\n",
    "        preds: predicted output for X\n",
    "    RETURNS: results as a dictionary of numpy arrays\n",
    "        mse: mean squared error for each column\n",
    "        rmse: rMSE for each column\n",
    "        fvaf: Fraction of Variance Accounted For, best is 1.0\n",
    "        r_squared: coeffcient of determination (CHECK: isn't this evar?)\n",
    "    '''\n",
    "    score = metrics.r2_score(y, preds, multioutput='variance_weighted')\n",
    "\n",
    "    mse, rmse = mse_rmse(y, preds)\n",
    "    evar = explained_variance_score(y, preds)\n",
    "    \n",
    "    #  This is a \n",
    "    # dictionary of numpy arrays. The numpy arrays are\n",
    "    # be row vectors, where each element is the result \n",
    "    # for a different output, when using multiple regression.\n",
    "    # The keys of the dictionary are the name of the performance \n",
    "    # metric, and the values are the numpy row vectors\n",
    "    results = {\n",
    "        'mse'  : np.reshape(mse,  (1, -1)), \n",
    "        'rmse' : np.reshape(rmse, (1, -1)), \n",
    "        'fvaf' : np.reshape(evar, (1, -1)),  # Fraction of Variance Accounted For\n",
    "        'r_squared': np.reshape(score,(1, -1)), \n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS0S51F7d6ru"
   },
   "source": [
    "# CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBwCLg43d6rv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class KFoldHolisticCrossValidation():\n",
    "    '''\n",
    "    Cross-validation class. This class will perform cross-validation across for a \n",
    "    single hyper-parameter set.\n",
    "    '''\n",
    "    def __init__(self, model, eval_func, rotation_skip=1): #opt_metric, \n",
    "                #maximize_opt_metric=False, rotation_skip=1):\n",
    "        ''' TODO\n",
    "        \n",
    "        :param model: The Scikit-Learn model to be trained\n",
    "        :param eval_func': Python function that will be used to evaluate a model\n",
    "                                parameters: (inputs, desired outputs, model predictions)\n",
    "        :param rotation_skip: Number of CV rotations for every one rotation to train & evaluate.  \n",
    "                                Typical is 1 (train and evaluate all rotations), but when we are \n",
    "                                debugging, it is helpful to perform a smaller number of train/evaluate\n",
    "                                cycles\n",
    "        '''\n",
    "        # TODO: set the class variables\n",
    "        self.model = #TODO\n",
    "        self.eval_func = #TODO\n",
    "        self.rotation_skip = rotation_skip\n",
    "\n",
    "    def perform_cross_validation(self, X, y, folds_idx, trainsize):\n",
    "        ''' TODO: \n",
    "        \n",
    "        This is where the bulk of the work will be done\n",
    "        Perform cross validation for a singular train set size and single \n",
    "        hyper-parameter set, by evaluating the model's performance over \n",
    "        multiple data set rotations all of the same size.\n",
    "\n",
    "        NOTE: This function assumes the hyper-parameters have already been \n",
    "              set in the model\n",
    "            \n",
    "        PARAMS:\n",
    "            X: numpy array containing all of the input data \n",
    "               (folds concatenated together)\n",
    "            y: numpy array containing all of the output data \n",
    "               (folds concatenated together)\n",
    "            folds_idx: list of lists containing the indexes\n",
    "                       of each element in each fold, eg\n",
    "                       folds_idx[i] = [start_idx, ... , end_idx]\n",
    "            trainsize: number of folds to use for training\n",
    "            \n",
    "        RETURNS: train, val, and test set results for all rotations of the  \n",
    "                 data sets and the summary (i.e., the averages over all the \n",
    "                 rotations) of the results. \n",
    "                 \n",
    "                 results is a dictionary of dictionaries of r-by-n numpy \n",
    "                 arrays, where r is the number of rotations, and n is the \n",
    "                 number of outputs from the model.\n",
    "                 \n",
    "                 summary is a dict of dictionaries of 1-by-n numpy arrays containing\n",
    "                 the mean and standard deviation of the metrics in results across\n",
    "                 all rotations\n",
    "                 \n",
    "                 In our BMI dataset, n = 2 (e.g., shoulder torque and elbow torque)\n",
    "\n",
    "                 General form:\n",
    "                     results.keys() = ['train', 'val', 'test']\n",
    "\n",
    "                     results['train'].keys() = ['metric1', 'metric2', ...]\n",
    "                     \n",
    "                     results['train']['metric1'] = numpy_array\n",
    "                     \n",
    "                     results = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse' : r_by_n_numpy_array,\n",
    "                                     'rmse': r_by_n_numpy_array, \n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "                     \n",
    "                     summary = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse_mean' : 1_by_n_numpy_array,\n",
    "                                     'mse_std'  : 1_by_n_numpy_array,\n",
    "                                     'rmse_mean': 1_by_n_numpy_array, \n",
    "                                     'rmse_std' : 1_by_n_numpy_array,\n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "\n",
    "                    For example, you can access the MSE results for the \n",
    "                    validation set like so:\n",
    "                        results['val']['mse'] \n",
    "                    For example, you can access the summary (i.e. the average \n",
    "                    results over all the rotations) for the test set for the\n",
    "                    rMSE like so:\n",
    "                        summary['test']['rmse_mean']                \n",
    "        '''\n",
    "        \n",
    "        # Verify that a valid train set size was provided\n",
    "        nfolds = len(folds_idx)\n",
    "        if trainsize > nfolds - 2: \n",
    "            err_msg = \"ERROR: KFoldHolisticCrossValidation.perform_cross_validation() - \"\n",
    "            err_msg += \"trainsize (%d) cant be more than nfolds (%d) - 2\" % (trainsize, nfolds)\n",
    "            raise ValueError(err_msg)\n",
    "        \n",
    "        # Set up results data structures for each rotation\n",
    "        results = {'train': None, 'val': None, 'test': None}\n",
    "        summary = {'train': {}, 'val': {}, 'test': {}}\n",
    "        \n",
    "        model = self.model\n",
    "        evaluate = self.eval_func\n",
    "        \n",
    "        # Rotate through different train, val, and test sets\n",
    "        for rotation in range(0, nfolds, self.rotation_skip):\n",
    "            (\n",
    "                Xtrain, ytrain, Xval, yval,  Xtest, ytest\n",
    "            ) = self.get_data(X, y, folds_idx, nfolds, rotation, trainsize)\n",
    "\n",
    "            print(ytrain.shape)\n",
    "            \n",
    "            # TODO: Train model using the training set\n",
    "            \n",
    "\n",
    "            # TODO: Predict with the model for train, val, and test sets\n",
    "            preds = #TODO\n",
    "            preds_val = #TODO\n",
    "            preds_test = #TODO\n",
    "\n",
    "            # TODO: Evaluate the model for each set\n",
    "            res_train = #TODO\n",
    "            res_val = #TODO\n",
    "            res_test = #TODO\n",
    "\n",
    "            # Record the train, val, and test set results. These are dicts \n",
    "            # of result metrics, returned by the evaluate function\n",
    "\n",
    "            if results['train'] is None: \n",
    "                # First rotation: initialize structures\n",
    "                results['train'] = #TODO\n",
    "                rresults['val'] = #TODO\n",
    "                results['test'] = res_test\n",
    "            else:\n",
    "                # Other rotations: add results to existing structures\n",
    "                for metric in res_train.keys():\n",
    "                    results['train'][metric] = np.append(results['train'][metric], res_train[metric], axis=0)\n",
    "                    results['val'][metric] = np.append(results['val'][metric], res_val[metric], axis=0)\n",
    "                    results['test'][metric] = np.append(results['test'][metric], res_test[metric], axis=0)\n",
    "\n",
    "        # Compute/record mean and standard deviation for the size for each metric\n",
    "        #  The mean is across the rotations\n",
    "        for metric in results['train'].keys():\n",
    "            for stat_set in ['train', 'val', 'test']:\n",
    "                summary[stat_set][metric+'_mean'] = np.mean(results[stat_set][metric], \n",
    "                                                            axis=0).reshape(1, -1)\n",
    "                summary[stat_set][metric+'_std'] = np.std(results[stat_set][metric], \n",
    "                                                          axis=0).reshape(1, -1)\n",
    "\n",
    "        return results, summary\n",
    "\n",
    "    def get_data(self, X, y, folds_idx, nfolds, rotation, trainsize):\n",
    "        '''TODO\n",
    "        Determines the fold indices for the train, val, and test set given\n",
    "        the total number of folds, rotation, and training set size.\n",
    "        Use these fold indices to get the training, validation, and test sets\n",
    "        from all_xfolds and all_folds\n",
    "        '''\n",
    "        # Determine folds to use \n",
    "        # (eg fold 1,2,3 for trainsize=3, rotation=1, nfolds=20)\n",
    "        trainfolds = (np.arange(trainsize) + rotation) % nfolds\n",
    "        valfold = (nfolds - 2 + rotation) % nfolds\n",
    "        testfold = (valfold + 1) % nfolds\n",
    "        #print(\"FOLD IDX\", len(folds_idx), len(folds_idx[0]))\n",
    "        \n",
    "        # Construct a list to serve as an index into X for our training\n",
    "        # samples. This will contain the index of each sample the training set\n",
    "        train_idx = []\n",
    "        for i in trainfolds:\n",
    "            # the + operator concatenates raw python lists\n",
    "            train_idx += folds_idx[i] \n",
    "\n",
    "        #print(len(train_idx))\n",
    "        # TODO: Construct train set by indexing into X and y with the indices of the\n",
    "        #  samples that belong to the training set\n",
    "        Xtrain = #TODO \n",
    "        ytrain = #TODO\n",
    "        \n",
    "        # TODO: Construct validation set using the valfold.\n",
    "        #       Hint: this is always one fold\n",
    "        Xval = #TODO\n",
    "        yval = #TODO\n",
    "\n",
    "        # TODO: Construct test set using the testfold\n",
    "        Xtest = #TODO\n",
    "        ytest = #TODO\n",
    "        \n",
    "        return Xtrain, ytrain, Xval, yval, Xtest, ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzmIoQXcd6rw"
   },
   "outputs": [],
   "source": [
    "class CrossValidationGridSearch():\n",
    "    '''\n",
    "    This class is responsible for performing a grid trainsizes x paramsets CV experiments.\n",
    "    For each grid point, N-fold crossvalidation is performed (with potential skips in the\n",
    "    possible rotations).\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, model, paramsets, eval_func, opt_metric, \n",
    "                 maximize_opt_metric=False, trainsizes=[1], rotation_skip=1,\n",
    "                ):\n",
    "        ''' \n",
    "        Class instance constructor\n",
    "        \n",
    "        :param model: Model to be learned\n",
    "        :param paramsets: List of dicts.  Every dict contains a set of hyper-parameters for use in\n",
    "                            one experiment\n",
    "        :param eval_func: Python function that will be used to evaluate a model\n",
    "                                parameters: (inputs, desired outputs, model predictions)\n",
    "        :param opt_metric: Optimization metric to be used\n",
    "        :param maximize_opt_metric: True -> best model has high value for performance metric; \n",
    "                                    False -> best model has low value\n",
    "        :param trainsizes: A list of training set sizes (in terms of number of folds\n",
    "        :param rotation_skip: Number of CV rotations for every one rotation to train & evaluate.  \n",
    "                                Typical is 1 (train and evaluate all rotations), but when we are \n",
    "                                debugging, it is helpful to perform a smaller number of train/evaluate\n",
    "                                cycles\n",
    "        '''\n",
    "        # TODO: set the class variables\n",
    "        self.model = model\n",
    "        self.paramsets = paramsets\n",
    "        self.trainsizes = trainsizes\n",
    "        self.eval_func = eval_func\n",
    "        self.opt_metric = opt_metric + '_mean'\n",
    "        self.maximize_opt_metric = maximize_opt_metric\n",
    "        self.rotation_skip = rotation_skip\n",
    "        \n",
    "        # Results attributes\n",
    "        # Full recording of all results for all paramsets, sizes, rotations,\n",
    "        # and metrics. This is a list of dictionaries for each paramset\n",
    "        self.results = []\n",
    "        # Validation summary report of all means and standard deviations for \n",
    "        # all metrics, for all paramsets, and sizes. This is a 3D s-by-r-by-p \n",
    "        # numpy array. Where s is the number of sizes, r the number of summary \n",
    "        # metrics +2, and p is the number of paramsets\n",
    "        self.report_by_size = None\n",
    "        # List of the indices of the best paramset for each size\n",
    "        self.best_param_inds = None\n",
    "\n",
    "    def load_checkpoint(self, fname):\n",
    "        ''' PROVIDED\n",
    "        Load a checkpoint file into self.results\n",
    "        \n",
    "        :param fname: Full name of the file to load the checkpoint from. \n",
    "        '''\n",
    "        if not os.path.exists(fname):\n",
    "            raise ValueError('File %s does not exist'%fname)\n",
    "        \n",
    "        with open(fname, 'rb') as f:\n",
    "            self.results = pkl.load(f)\n",
    "            \n",
    "    def dump_checkpoint(self, fname):\n",
    "        ''' PROVIDED\n",
    "        Write the current set of results to a checkpoint file\n",
    "        \n",
    "        :param fname: Full name of file to write checkpoint to\n",
    "        '''\n",
    "        with open(fname, 'wb') as f:\n",
    "            pkl.dump(self.results, f)\n",
    "            \n",
    "    def reset_results(self):\n",
    "        ''' PROVIDED\n",
    "        Reset the current set of results that are stored internally\n",
    "        '''\n",
    "        self.results = []\n",
    "    \n",
    "    def cross_validation_gridsearch(self, X, y, folds_idx, checkpoint_fname=None):\n",
    "        ''' TODO\n",
    "        Perform the grid search with the given data (X, y, folds_idx).  This grid search is\n",
    "        smart in that if a specific set of hyper-parameters have already been done (encoded in\n",
    "        checkpoing), then building and evaluating for those hyper-parameters will not be done \n",
    "        again.\n",
    "        \n",
    "        :param X: Full set of input features\n",
    "        :param y: Full set of desired outputs\n",
    "        :param folds_idx: List of row indices in X/y, one for each fold\n",
    "        :param checkpoint_fname: Name of the output checkpoint file.  If None, then not written\n",
    "                to file.\n",
    "        '''\n",
    "\n",
    "        cross_val = KFoldHolisticCrossValidation(\n",
    "            self.model, self.eval_func, \n",
    "            rotation_skip = self.rotation_skip\n",
    "        )        \n",
    "\n",
    "        # Iterate over the parameter sets\n",
    "        for params in self.paramsets:\n",
    "\n",
    "            # Check that we haven't already done this before (from our checkpoint)\n",
    "            if params in [r['params'] for r in self.results]:\n",
    "                print('already evaled:', params)\n",
    "                continue\n",
    "      \n",
    "            print('evaling on:', params)\n",
    "        \n",
    "            # Set up the results for these parametrs\n",
    "            param_results = []\n",
    "            param_summary = None\n",
    "\n",
    "            # Set the parameters in the model\n",
    "            self.model.set_params(**params)\n",
    "            \n",
    "            # Iterate over the different train set sizes\n",
    "            # Running cross-validation on thm\n",
    "            for size in self.trainsizes:\n",
    "                # TODO: Perform Cross-Validation\n",
    "                result, summary = #TODO\n",
    "                \n",
    "                # Append results in param_results\n",
    "                param_results.append(result)\n",
    "                \n",
    "                # Append the mean and standard deviation statistics (summary)\n",
    "                if param_summary is None: \n",
    "                    param_summary = summary\n",
    "                else:\n",
    "                    # For each metric measured, append the summary results\n",
    "                    for metric in summary['train'].keys():\n",
    "                        for stat_set in ['train', 'val', 'test']:\n",
    "                            param_summary[stat_set][metric] = np.append(\n",
    "                                param_summary[stat_set][metric], \n",
    "                                summary[stat_set][metric], \n",
    "                                axis=0\n",
    "                            )\n",
    "                            \n",
    "            # Add this param's results to this accumulating results\n",
    "            self.results.append({\n",
    "                'params' :params,\n",
    "                'results':param_results, \n",
    "                'summary':param_summary\n",
    "            })\n",
    "\n",
    "            # Write the checkpoint file\n",
    "            if checkpoint_fname is not None:\n",
    "                self.dump_checkpoint(checkpoint_fname)\n",
    "        \n",
    "\n",
    "    def get_reports_all(self):\n",
    "        ''' PROVIDED\n",
    "        Generate reports on the internally stored results\n",
    "        \n",
    "        :return: Dictionary containing two keys: 'report_by_size', 'best_param_inds'\n",
    "        '''\n",
    "        self.report_by_size = self.get_reports()\n",
    "        self.best_param_inds = self.get_best_params(\n",
    "            self.opt_metric, self.maximize_opt_metric\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'report_by_size' : self.report_by_size, \n",
    "            'best_param_inds': self.best_param_inds\n",
    "        }\n",
    "    \n",
    "    # Report generation code. Provided, but you should still read it\n",
    "    \"\"\" PROVIDED\n",
    "    Functions to generate a report of the result of the cross-validation\n",
    "    \"\"\"\n",
    "    def get_reports(self):\n",
    "        ''' PROVIDED\n",
    "        Get the mean validation summary of all the parameters for each size\n",
    "        for all metrics. This is used to determine the best parameter set  \n",
    "        for each size\n",
    "        \n",
    "        RETURNS: the report_by_size as a 3D s-by-r-by-p array. Where s is \n",
    "                 the number of train sizes tried, r is the number of summary  \n",
    "                 metrics evaluated+2, and p is the number of parameter sets.\n",
    "        '''\n",
    "        results = self.results\n",
    "        sizes = np.reshape(self.trainsizes, (1, -1))\n",
    "        \n",
    "        nsizes = sizes.shape[1]\n",
    "        nparams = len(results)\n",
    "        \n",
    "        # Set up the reports objects\n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics \n",
    "        report_by_size = np.empty((nsizes, len(colnames), nparams), dtype=object)\n",
    "\n",
    "        # Determine mean val for each paramset for each size for all metrics\n",
    "        for p, paramset_result in enumerate(results):\n",
    "            params = paramset_result['params']\n",
    "            res_val = paramset_result['summary']['val']\n",
    "\n",
    "            # Compute mean val result for each train size for each metric\n",
    "            means_by_size = [np.mean(res_val[metric], axis=1) \n",
    "                             for metric in metrics]\n",
    "            \n",
    "            print(\"MEAN\", means_by_size)\n",
    "            # Include the train set sizes into the report\n",
    "            means_by_size = np.append(sizes, means_by_size, axis=0)\n",
    "            print(\"SIZES\", sizes)\n",
    "            print(\"MEAN2\", means_by_size)\n",
    "            # Include the parameter sets into the report\n",
    "            param_strgs = np.reshape([str(params)]*nsizes, (1, -1))\n",
    "            means_by_size = np.append(param_strgs, means_by_size, axis=0).T\n",
    "            print(\"MEAN3\", means_by_size)\n",
    "            # Append the parameter set means into the report \n",
    "            report_by_size[:,:,p] = means_by_size\n",
    "        return report_by_size\n",
    "\n",
    "    def get_best_params(self, opt_metric, maximize_opt_metric):\n",
    "        ''' PROVIDED \n",
    "        (Do read through all the provided code)\n",
    "        Determines the best parameter set for each train size,  \n",
    "        based on a specific metric.\n",
    "        \n",
    "        PARAMS:\n",
    "            opt_metric: optimized metric. one of the metrics returned \n",
    "                        from eval_func, with '_mean' appended for the\n",
    "                        summary stat. This is the mean metric used to  \n",
    "                        determine the best parameter set for each size\n",
    "                        \n",
    "            maximize_opt_metric: True if the max of opt_metric should be\n",
    "                                 used to determine the best parameters.\n",
    "                                 False if the min should be used.\n",
    "        RETURNS: list of best parameter set indicies for each size \n",
    "        '''\n",
    "        results = self.results\n",
    "        report_by_size = self.report_by_size \n",
    "                \n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        \n",
    "        # Determine best params for each size, for the optimized metric\n",
    "        best_param_inds = None\n",
    "        metric_idx = metrics.index(opt_metric)\n",
    "        \n",
    "        # Report info for all paramsets for the optimized metric\n",
    "        report_opt_metric = report_by_size[:, metric_idx+2, :]\n",
    "        \n",
    "        if maximize_opt_metric:\n",
    "            # Add two for the additional cols for params and size\n",
    "            best_param_inds = np.argmax(report_opt_metric, axis=1)\n",
    "        else: \n",
    "            best_param_inds = np.argmin(report_opt_metric, axis=1)\n",
    "        # Return list of best params indices for each size\n",
    "        return best_param_inds\n",
    "    \n",
    "    def get_best_params_strings(self):\n",
    "        ''' PROVIDED\n",
    "        Generates a list of strings of the best params for each size\n",
    "        RETURNS: list of strings of the best params for each size\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        results = self.results\n",
    "        return [str(results[p]['params']) for p in best_param_inds]\n",
    "\n",
    "    def get_report_best_params_for_size(self, size):\n",
    "        ''' PROVIDED\n",
    "        Get the mean validation summary for the best parameter set \n",
    "        for a specific size for all metrics.\n",
    "        PARAMS:\n",
    "            size: index of desired train set size for the best  \n",
    "                  paramset to come from. Size here is the index in \n",
    "                  the trainsizes list, NOT the actual number of folds.\n",
    "        RETURNS: the best parameter report for the size as an s-by-m  \n",
    "                 dataframe. Where each row is for a different size, and \n",
    "                 each column is for a different summary metric.\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        report_by_size = self.report_by_size \n",
    "        \n",
    "        bp_index = best_param_inds[size]\n",
    "        size_len = len(size) if type(size) is list else 1\n",
    "                \n",
    "        metrics = list(self.results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics\n",
    "        report_best_params_for_size = pd.DataFrame(\n",
    "            report_by_size[size_idx].T[bp_index].reshape(size_len,-1),\n",
    "            columns=colnames\n",
    "        )\n",
    "        return report_best_params_for_size\n",
    "\n",
    "    \"\"\" PROVIDED\n",
    "    Plotting code to display the result of the gird search and cross-validation\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_cv(self, foldsindices, results, summary, metrics, size):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after perform_cross_validation(), \n",
    "        displaying the train and val set performances for each rotation \n",
    "        of the training set. \n",
    "        \n",
    "        PARAMS:\n",
    "            foldsindices: indices of the train sets tried\n",
    "            results: results from perform_cross_validation()\n",
    "            summary: mean and standard deviations of the results\n",
    "            metrics: list of result metrics to plot. Available metrics \n",
    "                     are the keys in the dict returned by eval_func\n",
    "            size: train set size\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(results['train'][metric], axis=1)\n",
    "            res_val = np.mean(results['val'][metric], axis=1)\n",
    "            #res_test = np.mean(results['test'][metric], axis=1)\n",
    "            # Plot\n",
    "            ax.plot(foldsindices, res_train, label='train')\n",
    "            ax.plot(foldsindices, res_val, label='val')\n",
    "            #ax.plot(foldsindices, res_test, label='test')\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[0].legend(loc='upper right')\n",
    "        axs[0].set(xlabel='Fold Index')\n",
    "        axs[0].set(title='Performance for Train Set Size ' + str(size))\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_param_train_val(self, metrics, paramidx=0, view_test=False):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), \n",
    "        displaying the mean (summary) train and val set performances \n",
    "        for each train set size.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std'\n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "            paramidx: parameter set index\n",
    "            view_test: flag to view the test set results\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "\n",
    "        summary = results[paramidx]['summary']\n",
    "        params = results[paramidx]['params']\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        #fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(summary['train'][metric], axis=1)\n",
    "            res_val = np.mean(summary['val'][metric], axis=1)\n",
    "            # Plot\n",
    "            ax.plot(sizes, res_train, label='train')\n",
    "            ax.plot(sizes, res_val, label='val')\n",
    "            if view_test:\n",
    "                res_test = np.mean(summary['test'][metric], axis=1)\n",
    "                ax.plot(sizes, res_test, label='test')\n",
    "            ax.set(ylabel=metric)\n",
    "            ax.set_xticks(sizes)\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title=str(params))\n",
    "        axs[0].legend(loc='upper right')\n",
    "        return fig, axs\n",
    "    \n",
    "    def plot_allparams_val(self, metrics):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), displaying  \n",
    "        mean (summary) validation set performances for each train size \n",
    "        for all parameter sets for the specified metrics.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std' \n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(10,6))\n",
    "        #fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            for p, param_results in enumerate(results):\n",
    "                summary = param_results['summary']\n",
    "                params = param_results['params']\n",
    "                # Compute the mean for multiple outputs\n",
    "                res_val = np.mean(summary['val'][metric], axis=1)                \n",
    "                ax.plot(sizes, res_val, label=str(params))\n",
    "            ax.set(ylabel=metric)\n",
    "            ax.set_xticks(sizes)\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title='Validation Performance')\n",
    "        axs[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_best_params_by_size(self):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), displaying \n",
    "        mean (summary) train and validation set performances for the best \n",
    "        parameter set for each train size for the optimized metric.\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        results = self.results\n",
    "        metric = self.opt_metric\n",
    "        best_param_inds = self.best_param_inds\n",
    "        sizes = np.array(self.trainsizes)\n",
    "\n",
    "        # Unique set of best params for the legend\n",
    "        unique_param_sets = np.unique(best_param_inds)\n",
    "        lgnd_params = [self.paramsets[p] for p in unique_param_sets]\n",
    "\n",
    "        # Initialize figure\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(10,6))\n",
    "        #fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "        set_names = ['train', 'val']\n",
    "\n",
    "        # Construct each subplot\n",
    "        for i, (ax, set_name) in enumerate(zip(axs, set_names)):\n",
    "            for p in unique_param_sets:\n",
    "                # Obtain indices of sizes this paramset was best for\n",
    "                param_size_inds = np.where(best_param_inds == p)[0]\n",
    "                param_sizes = sizes[param_size_inds]\n",
    "                param_sizes.sort()\n",
    "                # Compute the mean over multiple outputs for each size\n",
    "                param_summary = results[p]['summary'][set_name]\n",
    "                metric_scores = np.mean(param_summary[metric][param_size_inds,:], axis=1)\n",
    "                # Plot the param results for each size it was the best for\n",
    "                ax.scatter(param_sizes, metric_scores, s=120, marker=(p+2, 1))\n",
    "                ax.set_xticks(param_sizes)\n",
    "                #ax.grid(True)\n",
    "\n",
    "            set_name += ' Set Performance'\n",
    "            ax.set(ylabel=metric, title=set_name)\n",
    "\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].legend(lgnd_params, bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        return fig, axs\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R498w0V0d6ry"
   },
   "source": [
    "# PERFORM CROSS VALIDATION FOR ELASTICNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFPq3LXBd6rz",
    "outputId": "5d838b88-56cf-47e3-bed1-3f39cbc0fe2c"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Generate list of parameters to use for cross validation\n",
    "using generate_paramsets()\n",
    "\n",
    "Create a dictionary that contains parameter lists for the following parameters\n",
    "\n",
    "alpha: .001, .005, .025, 0.125\n",
    "l1_ratio: .001, .05, .1, .2\n",
    "max_iter: 10000\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# For testing purposes, you may want to make the parameter lists smaller\n",
    "\n",
    "param_lists = #TODO\n",
    "\n",
    "allparamsets = # TODO\n",
    "\n",
    "allparamsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtIulw5_d6r1"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Initialize the cross validation object. Use ElasticNet for the \n",
    "model, use train set sizes of 1/2/3/5/8/16, use rmse\n",
    "as the metric to optimize, and 1 for the skip. We want to minimize \n",
    "rmse thus set maximize_opt_metrix=False\n",
    "\"\"\"\n",
    "model = ElasticNet()\n",
    "trainsizes = #TODO\n",
    "opt_metric = #TODO\n",
    "maximize_opt_metric = #TODO\n",
    "skip = 4\n",
    "#skip = 1\n",
    "\n",
    "# Create the cross validation grid search object\n",
    "crossval = CrossValidationGridSearch(\n",
    "    model, allparamsets, score_eval, opt_metric,\n",
    "    maximize_opt_metric, trainsizes, skip\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icL9zlq5d6r1"
   },
   "outputs": [],
   "source": [
    "''' TODO\n",
    "Checkpoint file\n",
    "'''\n",
    "\n",
    "# if you are using COLAB, then use the following:\n",
    "#fullcvfname = '/content/drive/MyDrive/hw6_crossval_checkpoint.pkl'\n",
    "#  (if you do not choose this in COLAB, then you will lose your results\n",
    "#   when you shut down your virtual machine)\n",
    "\n",
    "# if you are using Jupyter, then you may want to store locally:\n",
    "fullcvfname = \"hw6_crossval_checkpoint.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSJ706lkd6r2",
    "outputId": "22a5e000-00a4-422d-9425-52409bef3370"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Execute the grid_cross_validation() procedure for all \n",
    "parameters and sizes\n",
    "\"\"\"\n",
    "# TODO: make sure this is set appropriately. True if you want to \n",
    "#       just always to run cross validation, false if you want \n",
    "#       to re-load a previous run\n",
    "# If you change the parameters but do not set force = True,\n",
    "# the model will not be re-run and your previously selected\n",
    "# parameters will be used\n",
    "#TODO\n",
    "force = False\n",
    "#force = True\n",
    "\n",
    "# Should we load the checkpoint file?\n",
    "if not force and os.path.exists(fullcvfname):\n",
    "    checkpoint = crossval.load_checkpoint(fullcvfname)\n",
    "\n",
    "# Use grid_cross_validation() to run the full cross \n",
    "#       validation procedure\n",
    "# Note: when testing, run this using small lists of parameters \n",
    "#       (e.g. of length 2 or 4) and/or small trainsize lists \n",
    "#       (e.g. [1, 2, 3, 4, 5])\n",
    "# Note: for the final submission, make sure to use the complete \n",
    "#       parameter set list and trainsize list provided/specified\n",
    "#       This will take some time (longer than an hour)\n",
    "    \n",
    "crossval.cross_validation_gridsearch(\n",
    "        MI, torque, folds_idx,\n",
    "        fullcvfname \n",
    "    )\n",
    "crossval_report = crossval.get_reports_all()\n",
    "    \n",
    "crossval_report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSJ706lkd6r2",
    "outputId": "22a5e000-00a4-422d-9425-52409bef3370"
   },
   "outputs": [],
   "source": [
    "# PROVIDED: EXECUTE CELL\n",
    "crossval.results[0]['summary']['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQH-Aolud6r2"
   },
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45evlpBXd6r3",
    "outputId": "9b0d2389-3b70-4e4f-c4f6-fc9c9d7c5920"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Obtain all the results for all parameters, for all sizes, for all\n",
    "rotations. This is the results attribute of the crossval object \n",
    "\"\"\"\n",
    "all_results = crossval.results\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dBm9Faxd6r4",
    "outputId": "66465669-d6ec-415c-e7a2-852c87de68e8"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display the keys of the results object\n",
    "\"\"\"\n",
    "all_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIvU6VpEd6r4",
    "outputId": "b403e80f-9f48-4eeb-a024-29b8cc8a2997"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Obtain and display the indices of the best parameters for each size  \n",
    "using either the best_params_inds attribute of the crossval object or \n",
    "'best_param_inds' item from the crossval_report dict\n",
    "\"\"\"\n",
    "best_param_inds = crossval_report['best_param_inds']\n",
    "best_param_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eT_9LeHd6r5",
    "outputId": "b72e2cd1-e209-4a3f-e411-ddc25ad4c6ba"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Display the list of the best parameter sets for each size. Use\n",
    "get_best_params_strings()\n",
    "\"\"\"\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDwQOqrdd6r6",
    "outputId": "63354e4a-204a-4d9c-f40d-7bc7cac1becf"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Obtain and dsplay the shape of the report of all the parameters'   \n",
    "mean results over all sizes and rotations. This is the report_by_size \n",
    "attribute of the crossval object. It is also stored within the \n",
    "'report_by_size' item of the crossval_report dict\n",
    "\"\"\"\n",
    "report = #TODO\n",
    "\n",
    "report.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "flspoXWJd6r7",
    "outputId": "5d293863-c055-4a27-f287-4e9738ed6df4"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n",
    "crossval.plot_best_params_by_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFZEKmqsd6r8"
   },
   "source": [
    "## Reflection\n",
    "Interpret the above results. What is are the best parameter choices?  How does this change with training set size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5L6HSpAFd6r9"
   },
   "source": [
    "TODO: Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gvLhRjKyd6r9",
    "outputId": "206a5971-936e-497c-80d5-cbbc75587e87"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Plot the average results (summary) over train set size for all \n",
    "parameter sets for the metrics 'rmse_mean' and 'fvaf_mean'\n",
    "for the train and val sets. Use plot_param_train_val(). \n",
    "view_test=False\n",
    "\"\"\"\n",
    "metrics_used = ['rmse_mean', 'fvaf_mean']\n",
    "nparamsets = len(allparamsets)\n",
    "for p in range(nparamsets):\n",
    "    crossval.plot_param_train_val(metrics_used, paramidx=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "IPYY0rgBd6r-",
    "outputId": "54805ced-5e9e-43e9-a6f8-b51ee3d132c9"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the validation results for all parameters over all train \n",
    "sizes, for the specified metrics. Use plot_allparams_val()\n",
    "\"\"\"\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "sVS59-Bsd6r_",
    "outputId": "81d7a63b-aafb-4afe-a3c1-b09dde9da4a7"
   },
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "For the best parameter set for the train set size at index 5,\n",
    "plot the TRAIN, VAL, and TEST set performances using \n",
    "plot_param_train_val() for just the optimized metric\n",
    "\"\"\"\n",
    "size_idx = 5\n",
    "#size_idx = 2\n",
    "bp_idx = best_param_inds[size_idx]\n",
    "crossval.plot_param_train_val([crossval.opt_metric], \n",
    "                              paramidx=bp_idx, view_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "A-xBPgnPd6r_",
    "outputId": "e69fd99d-c021-4a08-9cdf-40a1d174ab99"
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Use get_report_best_params_for_size() to display the report of  \n",
    "the average val statistics for the best parameter set, for the \n",
    "train set size at index 5 (i.e. size_idx)\n",
    "\"\"\"\n",
    "report_best_params = # TODO\n",
    "report_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAHvaTUDd6sA"
   },
   "source": [
    "## Reflection\n",
    "Consider the above results. Do the same set of parameters work best on all\n",
    "training set sizes? Explain why or why not this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSRlTMm-d6sA"
   },
   "source": [
    "TODO: answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSRlTMm-d6sA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework6_sol2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
